{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "\n",
    "from attention_utils import get_activations, get_data_recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 20\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    N = 300000\n",
    "    # N = 300 -> too few = no training\n",
    "    inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 20, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "    if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "        m = model_attention_applied_before_lstm()\n",
    "    else:\n",
    "        m = model_attention_applied_after_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 32)       4480        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 32, 20)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 20)       0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32, 20)       420         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 32)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 20, 32)       0           lstm_1[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 640)          0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            641         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,541\n",
      "Trainable params: 5,541\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/1\n",
      "270000/270000 [==============================] - 47s 173us/step - loss: 0.0558 - acc: 0.9729 - val_loss: 1.2143e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb33da3a9b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    m.fit([inputs_1], outputs, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00329404 0.00381582 0.0033327  0.00358431 0.00389199 0.00392618\n",
      " 0.00348918 0.00379463 0.00371543 0.00436569 0.5719843  0.00887022\n",
      " 0.34039593 0.01087286 0.00559155 0.00522652 0.00648651 0.00528309\n",
      " 0.00337977 0.00469928]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0015545  0.00220141 0.00176699 0.00173312 0.00167954 0.00212241\n",
      " 0.00207433 0.00214423 0.00155933 0.00177613 0.61657715 0.00370741\n",
      " 0.3448404  0.00379096 0.00214143 0.00240255 0.00195545 0.00206354\n",
      " 0.00191317 0.0019959 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.5811717e-04 8.4552565e-04 6.2766945e-04 7.1638147e-04 6.3873362e-04\n",
      " 6.9831859e-04 7.3113543e-04 8.1543199e-04 6.4646680e-04 8.4723427e-04\n",
      " 6.2292922e-01 1.7021801e-03 3.6241654e-01 1.7775133e-03 7.6632941e-04\n",
      " 7.1268145e-04 5.8719073e-04 6.0428795e-04 7.0957217e-04 6.6949433e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00145269 0.0016765  0.00150653 0.00168419 0.00161455 0.00144896\n",
      " 0.00166505 0.0019233  0.00215654 0.0025208  0.5947051  0.0061576\n",
      " 0.3581848  0.0069827  0.00245819 0.00228569 0.00598725 0.00211862\n",
      " 0.00191157 0.00155932]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00303294 0.00347129 0.0036502  0.00375578 0.00353463 0.00384537\n",
      " 0.00311059 0.00334849 0.00273571 0.00388588 0.6285368  0.00449569\n",
      " 0.30413085 0.00482569 0.0049308  0.00428061 0.00187651 0.00449265\n",
      " 0.00442345 0.00363602]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00350965 0.00345764 0.00270955 0.00294178 0.00281811 0.00526052\n",
      " 0.00307398 0.00334291 0.00361568 0.00451736 0.6021981  0.00773319\n",
      " 0.32333356 0.00622073 0.00426074 0.00530018 0.00568189 0.00304129\n",
      " 0.00337897 0.0036041 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00104974 0.00144748 0.00128191 0.00109342 0.00133149 0.00151568\n",
      " 0.00126256 0.00165845 0.00130524 0.00157468 0.6417538  0.00446819\n",
      " 0.32910395 0.00266218 0.00163754 0.00201681 0.00140281 0.00105244\n",
      " 0.00108948 0.00129218]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0018087  0.00365892 0.00273318 0.00273099 0.00289435 0.0025298\n",
      " 0.00306687 0.00435243 0.0022079  0.00283987 0.6190462  0.00455133\n",
      " 0.3274808  0.00436182 0.00409574 0.00207393 0.00150167 0.00244623\n",
      " 0.00373825 0.00188104]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00196738 0.00258917 0.002217   0.00204784 0.00183298 0.00322388\n",
      " 0.00268941 0.00253733 0.0016117  0.00215173 0.6204398  0.0029622\n",
      " 0.33845204 0.00296861 0.00272665 0.00176733 0.00123762 0.00187731\n",
      " 0.00221359 0.00248645]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0010761  0.00130547 0.00143056 0.00142492 0.00111752 0.00140508\n",
      " 0.00126191 0.0014979  0.00122367 0.00184693 0.6233827  0.00450764\n",
      " 0.34570336 0.00410392 0.00146528 0.00148025 0.00208559 0.00118111\n",
      " 0.00122655 0.00127358]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00152795 0.00239173 0.00200982 0.00281629 0.00157883 0.00254839\n",
      " 0.00167795 0.00225067 0.00135472 0.00168501 0.6198143  0.0030847\n",
      " 0.34222388 0.00365094 0.00279167 0.00206664 0.00124875 0.00186837\n",
      " 0.00183299 0.00157642]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00062899 0.00093716 0.00076282 0.00071211 0.00081638 0.00063667\n",
      " 0.00102182 0.00098031 0.00072317 0.00073876 0.62318355 0.0018211\n",
      " 0.35958326 0.00222797 0.00083035 0.00084303 0.00070046 0.00118796\n",
      " 0.00080986 0.00085427]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00135517 0.00166035 0.00165948 0.00210767 0.00182847 0.00177984\n",
      " 0.00179068 0.00193707 0.00127867 0.00233666 0.6180676  0.00282178\n",
      " 0.34736085 0.00275907 0.00256673 0.00156288 0.00163365 0.00136822\n",
      " 0.00211074 0.00201442]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00134793 0.00147083 0.00133201 0.00117889 0.00157584 0.00174102\n",
      " 0.00189198 0.00167387 0.00139189 0.00154921 0.6191513  0.00424096\n",
      " 0.34728158 0.00383285 0.00168211 0.00219528 0.00184433 0.0014808\n",
      " 0.00128737 0.00184991]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00243493 0.00275719 0.00253622 0.00355475 0.00227607 0.00244602\n",
      " 0.00250472 0.00269066 0.00318144 0.00326601 0.5666845  0.00721076\n",
      " 0.36436915 0.01053233 0.00400943 0.003764   0.00652524 0.00350793\n",
      " 0.00311298 0.00263566]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00090634 0.0012594  0.00094884 0.00128718 0.0009355  0.00121585\n",
      " 0.00120843 0.00115618 0.00089678 0.00106635 0.6207628  0.00169396\n",
      " 0.35776186 0.00233888 0.0010858  0.00106723 0.0009154  0.00097474\n",
      " 0.00163317 0.00088527]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00240061 0.00486492 0.00307555 0.0037337  0.00189024 0.00204095\n",
      " 0.00379135 0.00429093 0.00342331 0.00290799 0.60319763 0.0053405\n",
      " 0.32937044 0.00666905 0.00473458 0.00333835 0.00543208 0.00320443\n",
      " 0.00412488 0.00216856]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00090955 0.00130074 0.00111736 0.00124683 0.00102336 0.00111104\n",
      " 0.00123343 0.00129499 0.00094881 0.00112683 0.62244415 0.00214862\n",
      " 0.35387617 0.00314582 0.00127928 0.0010863  0.00082698 0.00157027\n",
      " 0.00125842 0.00105096]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00271775 0.00287341 0.00279134 0.00310261 0.00255824 0.00305212\n",
      " 0.00264019 0.0029486  0.00326723 0.00408082 0.58523405 0.00771478\n",
      " 0.3458547  0.00797094 0.00414524 0.00447611 0.00493372 0.00341987\n",
      " 0.00306465 0.00315364]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00106594 0.00163251 0.00136459 0.00155891 0.0012753  0.00167186\n",
      " 0.00147975 0.00159409 0.00103483 0.00154554 0.6227472  0.00284755\n",
      " 0.34966284 0.00238331 0.00173865 0.00134904 0.00107176 0.00104059\n",
      " 0.00133441 0.00160127]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00254854 0.00235549 0.00270897 0.00308521 0.00291858 0.00226208\n",
      " 0.00266274 0.00289201 0.00417633 0.00440226 0.5615324  0.0110546\n",
      " 0.35464466 0.01281337 0.00542858 0.00539536 0.00782022 0.00482713\n",
      " 0.0032391  0.00323242]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00123243 0.00243652 0.00171115 0.00184323 0.00138452 0.00142546\n",
      " 0.00226286 0.00291004 0.00149268 0.00163717 0.61692154 0.00354385\n",
      " 0.3464694  0.00387923 0.00245098 0.00181249 0.00183761 0.00156692\n",
      " 0.00183384 0.00134799]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00066846 0.00104055 0.00086307 0.00091628 0.00077015 0.00085358\n",
      " 0.00094463 0.00102697 0.00072492 0.0008666  0.62251914 0.00220244\n",
      " 0.3589319  0.00225073 0.00090123 0.00093701 0.00089071 0.00089881\n",
      " 0.00090703 0.00088578]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00102867 0.00116712 0.00105466 0.0011384  0.00102462 0.00101184\n",
      " 0.00115966 0.00109462 0.00091744 0.00106436 0.6217623  0.00203862\n",
      " 0.35501266 0.00284078 0.00107583 0.00130641 0.0009171  0.00179699\n",
      " 0.00120702 0.001381  ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00214981 0.00213981 0.00177394 0.00185944 0.00206872 0.00241837\n",
      " 0.00213554 0.00198374 0.00196093 0.0027379  0.64087766 0.00327679\n",
      " 0.31679395 0.0037668  0.00230084 0.0028201  0.00199805 0.00193157\n",
      " 0.0022152  0.00279088]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00137794 0.00182098 0.00164694 0.00175763 0.0014777  0.00159669\n",
      " 0.00190853 0.00184005 0.00136037 0.00204068 0.6221362  0.0026065\n",
      " 0.34477425 0.00350887 0.0018034  0.00149535 0.00108824 0.0019287\n",
      " 0.00222422 0.00160676]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00067345 0.0007301  0.00064374 0.00077002 0.00084878 0.00073681\n",
      " 0.00085921 0.00077418 0.00076752 0.00079501 0.6221753  0.00155998\n",
      " 0.36127743 0.00189481 0.0009129  0.00104621 0.00064211 0.00109899\n",
      " 0.00089462 0.00089884]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00230621 0.00301346 0.00210281 0.0031723  0.00243005 0.0025936\n",
      " 0.0033899  0.00293754 0.00218121 0.00181557 0.61635256 0.00380759\n",
      " 0.33009797 0.00435412 0.00270462 0.00360536 0.00469923 0.00256431\n",
      " 0.00384463 0.00202696]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00080553 0.00140846 0.0009922  0.00102583 0.00095816 0.00112369\n",
      " 0.00116905 0.00135009 0.00093827 0.00098775 0.61868465 0.00211308\n",
      " 0.35962963 0.00218743 0.00125861 0.0013179  0.00095374 0.00102924\n",
      " 0.00121273 0.00085399]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00065754 0.00093527 0.00086289 0.00082626 0.00067381 0.00082636\n",
      " 0.00084651 0.00095004 0.00067735 0.00077834 0.62245476 0.00170688\n",
      " 0.3609717  0.00230764 0.00079688 0.0008128  0.00065722 0.000815\n",
      " 0.0007861  0.00065669]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00110815 0.00121872 0.00123173 0.00166556 0.00117545 0.00164307\n",
      " 0.00124588 0.00131191 0.00105598 0.00171738 0.62300634 0.00246105\n",
      " 0.35088122 0.00250829 0.00161302 0.00143049 0.00089913 0.00102422\n",
      " 0.00136946 0.00143297]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00092869 0.00117405 0.00132788 0.00151399 0.00105271 0.00121591\n",
      " 0.00128604 0.00140683 0.00090686 0.00131131 0.62296575 0.00249641\n",
      " 0.35235453 0.00303475 0.00147535 0.00100411 0.0007852  0.00133736\n",
      " 0.00122596 0.00119632]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00142184 0.00301999 0.00301832 0.00318288 0.00183963 0.00203441\n",
      " 0.00207277 0.00312916 0.00155807 0.00217256 0.6126482  0.00497497\n",
      " 0.33918193 0.00500008 0.00383985 0.00225463 0.00243616 0.00231539\n",
      " 0.00217705 0.00172212]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00167601 0.00191945 0.0017348  0.00180192 0.00205126 0.00192262\n",
      " 0.00180358 0.00207128 0.00250406 0.00261352 0.59642315 0.00647456\n",
      " 0.35368186 0.00701757 0.00291822 0.00313344 0.00405321 0.0023624\n",
      " 0.00194803 0.00188909]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00107804 0.00149926 0.00106767 0.00101347 0.00108804 0.00135449\n",
      " 0.00144992 0.00148147 0.0012853  0.00116852 0.64460516 0.002991\n",
      " 0.32882988 0.00231066 0.00154899 0.00215484 0.00151557 0.00114831\n",
      " 0.00113499 0.00127443]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00091372 0.00109537 0.00104799 0.00109673 0.00090584 0.00104314\n",
      " 0.00097719 0.00105393 0.00093986 0.00117645 0.62248695 0.00210132\n",
      " 0.35587603 0.00295661 0.00112553 0.00105208 0.00082658 0.00122243\n",
      " 0.00107918 0.00102307]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00270405 0.00323223 0.00240612 0.00193875 0.00341148 0.00389657\n",
      " 0.00317841 0.00307114 0.00266793 0.00291327 0.61253643 0.00684576\n",
      " 0.32573488 0.00437952 0.00313303 0.00543577 0.00383624 0.00256859\n",
      " 0.00255622 0.00355362]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00105392 0.00120869 0.00132737 0.00128202 0.00088161 0.0013527\n",
      " 0.00118894 0.00127792 0.00100476 0.00125676 0.6431293  0.00246422\n",
      " 0.33096746 0.00282415 0.00185861 0.00150983 0.00151966 0.001603\n",
      " 0.00108516 0.00120396]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.8972195e-04 8.8630593e-04 6.5267470e-04 6.8715750e-04 6.8530184e-04\n",
      " 7.9294993e-04 8.9646073e-04 8.8262989e-04 6.6908821e-04 7.0968154e-04\n",
      " 6.2214822e-01 1.8817906e-03 3.6235142e-01 1.5954884e-03 8.2149904e-04\n",
      " 8.2607724e-04 7.6454948e-04 7.1271847e-04 7.0351729e-04 7.4279454e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00068993 0.00078924 0.00083028 0.00099777 0.00064938 0.00088854\n",
      " 0.00084157 0.00079974 0.0006382  0.0007596  0.6216281  0.00148862\n",
      " 0.36193117 0.00209392 0.00087681 0.00080256 0.00077052 0.00089792\n",
      " 0.00085619 0.00076988]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00114609 0.00122615 0.00127601 0.00139278 0.00111754 0.00150181\n",
      " 0.00121572 0.00124692 0.00117177 0.00148903 0.65167576 0.00302937\n",
      " 0.3205936  0.00289464 0.00175753 0.00182931 0.00136065 0.00139345\n",
      " 0.00116246 0.00151938]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0012523  0.00146681 0.00161828 0.00155614 0.00150202 0.0013057\n",
      " 0.00143685 0.00160185 0.00168586 0.00205116 0.6149601  0.00659693\n",
      " 0.34438917 0.00597373 0.00192566 0.0021254  0.00355019 0.00190524\n",
      " 0.00135826 0.00173837]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00198945 0.00246098 0.00238439 0.00201465 0.00218772 0.00306129\n",
      " 0.0023126  0.00267995 0.00200812 0.0027204  0.61596847 0.00796263\n",
      " 0.33044845 0.00460378 0.00318843 0.0035231  0.00358507 0.00215766\n",
      " 0.00194892 0.00279397]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.8132596e-04 8.2454470e-04 6.9934246e-04 9.1174850e-04 5.5917783e-04\n",
      " 8.0150005e-04 7.9466827e-04 8.0276554e-04 5.3795171e-04 6.3134969e-04\n",
      " 6.2281013e-01 1.2417892e-03 3.6310130e-01 1.7412328e-03 7.6531968e-04\n",
      " 6.5540255e-04 5.2380556e-04 6.7720364e-04 7.2316587e-04 6.1623356e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00215686 0.00245443 0.00211191 0.00131131 0.00189636 0.00275616\n",
      " 0.00212046 0.00226322 0.00185509 0.00209636 0.62043846 0.00622884\n",
      " 0.3329961  0.00416909 0.00222376 0.00326515 0.00241471 0.00279402\n",
      " 0.00145676 0.00299101]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00074838 0.00125209 0.00091104 0.00107441 0.0009474  0.00107822\n",
      " 0.00109396 0.00124413 0.00078174 0.00083104 0.6188902  0.0018609\n",
      " 0.3610201  0.00186068 0.00129018 0.00117575 0.00097929 0.00108821\n",
      " 0.00093526 0.00093702]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00111137 0.0012655  0.00137426 0.00114109 0.00112352 0.00148342\n",
      " 0.00146268 0.00155881 0.00108699 0.00137908 0.6202031  0.00322205\n",
      " 0.35208237 0.0028269  0.00164629 0.00149143 0.0015175  0.00147003\n",
      " 0.00113996 0.00141371]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00137743 0.00150608 0.00142447 0.00161931 0.00142556 0.00187237\n",
      " 0.00133715 0.00137291 0.00132453 0.00214265 0.6429777  0.00327951\n",
      " 0.32500798 0.00275596 0.00210761 0.00190111 0.00185267 0.00116428\n",
      " 0.00135947 0.00219126]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [4.4181111e-04 6.5779092e-04 5.4548460e-04 6.1902654e-04 5.2367465e-04\n",
      " 5.2788103e-04 7.0819713e-04 6.5350905e-04 4.7548822e-04 5.0080183e-04\n",
      " 6.2321430e-01 1.2132346e-03 3.6494702e-01 1.4755200e-03 5.8230915e-04\n",
      " 6.1548821e-04 4.9105438e-04 6.6774106e-04 5.8158487e-04 5.5806682e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00123272 0.00169478 0.00144538 0.0017258  0.00143625 0.00152399\n",
      " 0.00176743 0.00198372 0.00159424 0.0020595  0.6092868  0.00359831\n",
      " 0.3553036  0.00435942 0.00215205 0.00152742 0.00261029 0.00139803\n",
      " 0.00181808 0.00148218]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00096529 0.00122648 0.00099067 0.00106024 0.00091246 0.00118346\n",
      " 0.00124974 0.00110897 0.00083205 0.00089466 0.6205476  0.00246975\n",
      " 0.35702372 0.00210814 0.00093627 0.00202532 0.00127639 0.00104478\n",
      " 0.00094103 0.00120303]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.4130860e-04 8.1078225e-04 7.6312065e-04 8.3322398e-04 6.4113981e-04\n",
      " 6.6285988e-04 8.5414888e-04 8.6973468e-04 5.4592430e-04 6.7778851e-04\n",
      " 6.2230456e-01 1.3375393e-03 3.6315066e-01 1.7822136e-03 7.4346602e-04\n",
      " 6.4802857e-04 6.7305681e-04 7.3024025e-04 8.1741082e-04 6.1275437e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00074017 0.00105222 0.00087496 0.00101727 0.00080217 0.00086596\n",
      " 0.00090512 0.00101935 0.00082203 0.00094123 0.62082    0.00192158\n",
      " 0.36026716 0.00228167 0.00108079 0.00093391 0.00081747 0.00096671\n",
      " 0.00101256 0.00085769]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00336089 0.00486732 0.00546496 0.00518085 0.00490431 0.00370393\n",
      " 0.00564948 0.0094071  0.00546334 0.00670853 0.5385475  0.01133533\n",
      " 0.3282621  0.01299678 0.02274707 0.00469965 0.00760467 0.00900531\n",
      " 0.00653998 0.0035509 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00133426 0.00293328 0.00244589 0.00186607 0.00185464 0.00278465\n",
      " 0.00280497 0.00360636 0.00144684 0.00179612 0.61850494 0.00480466\n",
      " 0.33906627 0.00248465 0.00295931 0.00188504 0.00219255 0.00152496\n",
      " 0.00205597 0.00164857]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.0782040e-04 6.9530745e-04 5.7660276e-04 6.1100803e-04 5.8877055e-04\n",
      " 5.6168757e-04 6.1524147e-04 6.8930734e-04 6.1489735e-04 6.2927912e-04\n",
      " 6.2241107e-01 1.4035064e-03 3.6453518e-01 1.7384799e-03 7.0245273e-04\n",
      " 6.4562884e-04 5.4587115e-04 7.1948173e-04 6.7058852e-04 5.3782616e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00121585 0.00145919 0.00142737 0.00141987 0.00138739 0.00154137\n",
      " 0.00140118 0.00144721 0.00129602 0.00195965 0.65040755 0.0036146\n",
      " 0.3182687  0.00283794 0.00194949 0.00188421 0.00190382 0.00129128\n",
      " 0.00128638 0.00200097]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00195396 0.0026731  0.00228114 0.00214297 0.00218625 0.00239771\n",
      " 0.00237489 0.00244259 0.00202855 0.00258023 0.65301764 0.00438868\n",
      " 0.2999362  0.00403302 0.00278088 0.00292182 0.00211137 0.00268299\n",
      " 0.00261671 0.00244924]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00090222 0.0010914  0.00104278 0.00128998 0.00096697 0.00100322\n",
      " 0.00113764 0.00113765 0.00101168 0.00116984 0.6188532  0.00236871\n",
      " 0.35704774 0.00350814 0.00125273 0.00119744 0.00143812 0.00132988\n",
      " 0.0011813  0.00106936]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00142045 0.00190767 0.00150715 0.00196456 0.00132973 0.00150862\n",
      " 0.00222142 0.00196526 0.00169616 0.00172339 0.61122704 0.00349106\n",
      " 0.35026705 0.00524868 0.00216184 0.00205711 0.00271443 0.00212664\n",
      " 0.0020088  0.00145291]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00095482 0.00164851 0.00113106 0.00238662 0.00083434 0.00137151\n",
      " 0.00123428 0.00118175 0.0008593  0.00084253 0.6189275  0.00155344\n",
      " 0.35730386 0.00216416 0.00179598 0.00145987 0.00111006 0.00098795\n",
      " 0.0012702  0.00098223]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00076555 0.00147893 0.00104257 0.00108251 0.00095808 0.0010185\n",
      " 0.00124581 0.00135265 0.00082253 0.00098235 0.62202764 0.00210265\n",
      " 0.35689846 0.00247743 0.00100784 0.0008919  0.0008356  0.00089909\n",
      " 0.00126204 0.0008479 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00229704 0.00309207 0.00314764 0.00335617 0.00234616 0.0033313\n",
      " 0.003087   0.00320927 0.0018336  0.00228263 0.6173425  0.00357712\n",
      " 0.3291546  0.0035237  0.004235   0.00315885 0.00210402 0.00308382\n",
      " 0.00310394 0.0027335 ]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [7.9956604e-04 1.9211057e-03 9.6590060e-04 1.2744106e-03 9.1706601e-04\n",
      " 1.4318917e-03 1.3186226e-03 1.5980511e-03 8.4921304e-04 7.4729632e-04\n",
      " 6.2256950e-01 1.7001566e-03 3.5640904e-01 1.6249249e-03 1.4452050e-03\n",
      " 1.0498192e-03 5.6833390e-04 8.3553902e-04 1.2235080e-03 7.5087900e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [6.1667716e-04 7.7585160e-04 6.9586007e-04 9.0751151e-04 7.0260506e-04\n",
      " 7.6121686e-04 7.2932529e-04 7.3370902e-04 6.1669096e-04 7.8310841e-04\n",
      " 6.2282264e-01 1.4852326e-03 3.6222520e-01 1.6635890e-03 8.6306583e-04\n",
      " 8.9145557e-04 4.6748979e-04 7.3473330e-04 7.2215719e-04 8.0189179e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00153989 0.00193761 0.00184221 0.001774   0.00226774 0.00170874\n",
      " 0.00210663 0.00195453 0.00145463 0.00144719 0.623482   0.00316613\n",
      " 0.3352852  0.00319831 0.00269863 0.00410154 0.00202518 0.00384749\n",
      " 0.00171146 0.00245089]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0011819  0.00138633 0.00119369 0.00137766 0.00118645 0.00174655\n",
      " 0.00124213 0.00132809 0.00115941 0.00143129 0.6234529  0.00310538\n",
      " 0.34959966 0.00219415 0.00179677 0.00176707 0.00115674 0.00091575\n",
      " 0.00106481 0.00171333]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00162923 0.00156507 0.00178891 0.00211658 0.00200177 0.00156089\n",
      " 0.00170622 0.00190907 0.0022369  0.00292128 0.59905237 0.00585033\n",
      " 0.35214156 0.0070764  0.00360503 0.00237649 0.00330655 0.00292353\n",
      " 0.00221434 0.00201748]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00632706 0.00406042 0.00564885 0.003482   0.00419857 0.00704657\n",
      " 0.00979885 0.0085727  0.00518154 0.00858223 0.5414687  0.01199776\n",
      " 0.3065585  0.01133144 0.01084115 0.00753402 0.02254025 0.00988771\n",
      " 0.00465424 0.01028741]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00137647 0.00165902 0.00142894 0.00126116 0.00160406 0.00187559\n",
      " 0.00162309 0.00181511 0.00142117 0.00214437 0.62550193 0.00471348\n",
      " 0.33948317 0.00292186 0.00191304 0.00196361 0.0025411  0.00119821\n",
      " 0.00112323 0.00243135]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00078324 0.00113405 0.00083126 0.00106543 0.00074996 0.00097431\n",
      " 0.00106007 0.00104895 0.00076428 0.00079873 0.6207098  0.00163332\n",
      " 0.36057723 0.00227444 0.00098524 0.00094596 0.00091766 0.00095233\n",
      " 0.00103805 0.0007557 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00167172 0.00131456 0.0014666  0.00131055 0.00121706 0.00218226\n",
      " 0.00130349 0.00125777 0.00148909 0.00171041 0.6185796  0.00389152\n",
      " 0.34644902 0.00300414 0.00200484 0.00336044 0.00258342 0.00179641\n",
      " 0.00119094 0.00221614]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00115035 0.0013698  0.00124533 0.00145677 0.00120506 0.00128357\n",
      " 0.00126716 0.00141272 0.00132978 0.00140311 0.6139256  0.00304343\n",
      " 0.3561101  0.00358226 0.00189278 0.00187106 0.00207849 0.00168629\n",
      " 0.0013515  0.00133488]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00214533 0.0028092  0.00215902 0.00205001 0.00191536 0.00208058\n",
      " 0.00271371 0.00280389 0.00294412 0.00279069 0.5739811  0.00872082\n",
      " 0.35902774 0.00866601 0.00283144 0.00372867 0.01046425 0.00332993\n",
      " 0.0022694  0.00256868]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00205802 0.00274832 0.00164821 0.00266511 0.0021409  0.00269023\n",
      " 0.00239346 0.00239644 0.00226231 0.00188674 0.6149444  0.00318691\n",
      " 0.34141636 0.0029984  0.00303638 0.00358696 0.00160027 0.00176762\n",
      " 0.00285985 0.00171316]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00194297 0.00244059 0.00191837 0.0016052  0.00173372 0.00243227\n",
      " 0.00188866 0.00217658 0.00196156 0.00205121 0.61500573 0.00607785\n",
      " 0.33986503 0.00467891 0.00211938 0.00368493 0.00255885 0.00210207\n",
      " 0.00167007 0.00208614]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00209572 0.00254348 0.00181495 0.00142699 0.00310875 0.00283813\n",
      " 0.00243133 0.0025655  0.00246691 0.00267705 0.6236979  0.00570776\n",
      " 0.32685718 0.00341584 0.00315943 0.00382922 0.0019253  0.00223002\n",
      " 0.00209292 0.00311561]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00111849 0.00130413 0.00122213 0.00132226 0.0010816  0.00136862\n",
      " 0.00119278 0.00134433 0.00108376 0.00160407 0.6220348  0.00288834\n",
      " 0.35137653 0.00292608 0.00146194 0.00155187 0.00113819 0.001167\n",
      " 0.00114268 0.00167034]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00235943 0.00240584 0.00243951 0.00320892 0.00239619 0.00274286\n",
      " 0.0022423  0.00251612 0.0025268  0.00303035 0.62790596 0.00476798\n",
      " 0.31703976 0.00537572 0.00403858 0.00395832 0.00255287 0.00293527\n",
      " 0.00287807 0.00267915]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00109904 0.00136729 0.00111429 0.00126864 0.00124565 0.00147353\n",
      " 0.00142582 0.00132871 0.00107073 0.00125993 0.6219456  0.00345964\n",
      " 0.351669   0.00229332 0.00139406 0.00155116 0.00107569 0.00119434\n",
      " 0.00119628 0.00156723]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00093395 0.00121817 0.00126575 0.00148127 0.00111002 0.00139774\n",
      " 0.00138891 0.0015087  0.00086349 0.00136845 0.622871   0.00220907\n",
      " 0.35256398 0.00265762 0.00175492 0.00090718 0.00067836 0.00108583\n",
      " 0.00110606 0.00162952]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00068211 0.00128994 0.0010024  0.00097833 0.00107625 0.00105777\n",
      " 0.00106428 0.00136573 0.00079304 0.00106196 0.6235173  0.002633\n",
      " 0.3561686  0.00204022 0.00109517 0.00092549 0.00064811 0.00075737\n",
      " 0.00096404 0.00087888]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [6.0185650e-04 1.0014693e-03 7.7222846e-04 8.3487679e-04 6.7618286e-04\n",
      " 8.5757324e-04 9.0507430e-04 1.0260441e-03 6.5001904e-04 7.4376236e-04\n",
      " 6.2226200e-01 1.6685976e-03 3.6156237e-01 1.9030038e-03 8.7008066e-04\n",
      " 7.5075059e-04 6.5609528e-04 7.4557343e-04 8.7644334e-04 6.3591776e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00205874 0.00213947 0.00217404 0.00192254 0.00205801 0.00288573\n",
      " 0.00222175 0.00221952 0.00173492 0.00221924 0.62263656 0.00488194\n",
      " 0.33147243 0.00345856 0.00247628 0.00356822 0.00288314 0.00217703\n",
      " 0.00169491 0.00311699]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00124307 0.00251571 0.0015682  0.00214534 0.00099737 0.00167319\n",
      " 0.00176755 0.00176337 0.00144772 0.00135896 0.6177286  0.00258227\n",
      " 0.3501444  0.00284092 0.00243392 0.00170588 0.00144516 0.00155667\n",
      " 0.00168504 0.00139667]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00219509 0.00198568 0.0021307  0.00234428 0.00213377 0.00253774\n",
      " 0.00210704 0.00211507 0.00223284 0.00319628 0.62383413 0.0051563\n",
      " 0.32446766 0.00449988 0.00388547 0.00312312 0.00411497 0.00232943\n",
      " 0.00211968 0.00349089]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.3936493e-04 6.6859199e-04 5.9328263e-04 6.1977294e-04 5.9435342e-04\n",
      " 5.6608603e-04 6.7528989e-04 7.2458235e-04 5.8753998e-04 6.5501628e-04\n",
      " 6.2253183e-01 1.4331156e-03 3.6405170e-01 1.7604395e-03 7.0300751e-04\n",
      " 6.7432958e-04 5.2061700e-04 8.0364011e-04 6.8300217e-04 6.1440718e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [4.8525396e-04 6.8509788e-04 6.6581951e-04 5.9395912e-04 6.5714261e-04\n",
      " 5.4648652e-04 7.3855952e-04 7.4754673e-04 5.6161353e-04 6.5405294e-04\n",
      " 6.2297070e-01 1.9426413e-03 3.6254147e-01 1.8309742e-03 7.3270145e-04\n",
      " 7.0311164e-04 7.4983714e-04 8.2883402e-04 5.9084606e-04 7.7330542e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00166845 0.00204801 0.00186599 0.00132426 0.00163911 0.00189771\n",
      " 0.00177699 0.00210791 0.00192945 0.00190379 0.6270199  0.00643727\n",
      " 0.33035985 0.00382017 0.00248523 0.0031716  0.00302084 0.00216629\n",
      " 0.00135765 0.00199956]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00116221 0.00185959 0.00177472 0.00207974 0.00145165 0.00171733\n",
      " 0.00202615 0.00202114 0.00106351 0.00131674 0.6193361  0.00242709\n",
      " 0.34896064 0.00245891 0.00212384 0.00168173 0.00150611 0.00165602\n",
      " 0.001853   0.00152378]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00203684 0.00225401 0.00213382 0.00274307 0.0023279  0.00239467\n",
      " 0.00231115 0.00255784 0.00257938 0.00351188 0.59478235 0.00443397\n",
      " 0.35132962 0.00587575 0.004315   0.00290188 0.00314418 0.00288636\n",
      " 0.00311946 0.00236081]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [4.2067596e-04 5.8760785e-04 4.7966771e-04 4.9048074e-04 5.2522478e-04\n",
      " 4.6636903e-04 5.3951412e-04 5.6975381e-04 4.7949862e-04 5.2671385e-04\n",
      " 6.2286931e-01 1.2738139e-03 3.6613947e-01 1.4293175e-03 5.6017993e-04\n",
      " 5.5053749e-04 4.5984186e-04 5.9603411e-04 5.3264294e-04 5.0329731e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00220278 0.00259149 0.00197144 0.00251986 0.00262927 0.00266905\n",
      " 0.00242122 0.00252404 0.0028706  0.00279921 0.5882262  0.00663524\n",
      " 0.34942758 0.00645282 0.00439843 0.00439625 0.00665802 0.00325483\n",
      " 0.00241775 0.00293389]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00277419 0.00312661 0.00301302 0.00311782 0.00338501 0.00359416\n",
      " 0.00358246 0.00334953 0.00281879 0.00379094 0.60102475 0.00654368\n",
      " 0.32988477 0.00546651 0.00424439 0.00470157 0.00385306 0.0041385\n",
      " 0.00370372 0.00388658]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0007761  0.00107848 0.00098729 0.00093005 0.00073936 0.00087681\n",
      " 0.00109209 0.00106258 0.00077933 0.00088925 0.62055635 0.0019741\n",
      " 0.3599081  0.00252388 0.00089629 0.00095057 0.00113075 0.00103176\n",
      " 0.00094713 0.00086969]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00097067 0.00112382 0.00095896 0.00122039 0.00092599 0.00110668\n",
      " 0.0010332  0.00109336 0.0010497  0.00122435 0.62258327 0.00218783\n",
      " 0.35421646 0.00353766 0.00119479 0.00123316 0.00100741 0.00114092\n",
      " 0.00107344 0.00111787]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00126012 0.00148735 0.00157226 0.0014464  0.00092156 0.00210171\n",
      " 0.001619   0.00165692 0.00104684 0.0013187  0.6205017  0.00246553\n",
      " 0.35201544 0.00255962 0.00161486 0.00134601 0.00129475 0.00115333\n",
      " 0.00129222 0.00132572]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00125417 0.00276032 0.00324059 0.00259046 0.00178466 0.00151602\n",
      " 0.00220238 0.00308855 0.0014047  0.00194623 0.6201282  0.00507546\n",
      " 0.33537155 0.00447599 0.00307927 0.0017633  0.00241087 0.00216601\n",
      " 0.00208815 0.00165309]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [4.2463906e-04 5.8814109e-04 5.6841347e-04 6.6676061e-04 4.2423594e-04\n",
      " 5.3561322e-04 6.6111668e-04 6.4694486e-04 4.1123235e-04 5.1469461e-04\n",
      " 6.2289000e-01 1.0471932e-03 3.6594850e-01 1.4705295e-03 5.7368237e-04\n",
      " 5.0692738e-04 5.1831396e-04 5.5200350e-04 5.7633728e-04 4.7469721e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00093903 0.00120674 0.00094077 0.00115838 0.00103413 0.00096123\n",
      " 0.00130248 0.00135234 0.00114837 0.00121621 0.6138877  0.00222334\n",
      " 0.36120033 0.00309211 0.00165984 0.00132071 0.00169336 0.00130681\n",
      " 0.00125785 0.00109827]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00065601 0.00099958 0.00080645 0.00091471 0.00066275 0.00077256\n",
      " 0.00094737 0.00089567 0.00066377 0.00073827 0.6222054  0.00167033\n",
      " 0.3610283  0.00220888 0.00079804 0.00080029 0.00077212 0.00088122\n",
      " 0.00085665 0.00072166]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00121126 0.00167322 0.00158547 0.00179804 0.00120441 0.00137924\n",
      " 0.00161627 0.00168733 0.00118074 0.00141609 0.61849344 0.00390189\n",
      " 0.34888968 0.00402088 0.00176206 0.00166869 0.00192367 0.00165031\n",
      " 0.00153663 0.0014007 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00086399 0.00130705 0.00106917 0.00114149 0.00118923 0.00111238\n",
      " 0.00147048 0.00136316 0.00093486 0.00101372 0.62162375 0.00246714\n",
      " 0.3545934  0.00262912 0.00124013 0.00125919 0.00111353 0.00120928\n",
      " 0.00112963 0.00126934]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00097461 0.00116601 0.00133546 0.00127863 0.00106059 0.00105663\n",
      " 0.00113821 0.00128285 0.00111155 0.00142049 0.6203995  0.00346295\n",
      " 0.35178757 0.0042656  0.0014394  0.00141014 0.00152837 0.00156339\n",
      " 0.00114978 0.00116836]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00125046 0.0013795  0.00147744 0.0011875  0.00145622 0.00131595\n",
      " 0.00190013 0.00162282 0.00127279 0.00143729 0.6233976  0.00288448\n",
      " 0.34655935 0.00278297 0.0017406  0.00159508 0.00144072 0.00190412\n",
      " 0.00140157 0.00199343]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00212493 0.00174355 0.00174526 0.00223046 0.00205527 0.00151737\n",
      " 0.00183238 0.00186643 0.00273715 0.0029739  0.5996221  0.00476685\n",
      " 0.3498549  0.00824892 0.0029638  0.00276444 0.00330406 0.00297159\n",
      " 0.00239042 0.00228627]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00123888 0.00137429 0.00151445 0.00158806 0.00112861 0.00128352\n",
      " 0.00153972 0.00166536 0.00124473 0.00147569 0.615308   0.00367449\n",
      " 0.3532731  0.0040025  0.0017427  0.00161798 0.00190497 0.00158633\n",
      " 0.00143052 0.0014061 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00309422 0.00327879 0.00385041 0.00329879 0.0027909  0.00390147\n",
      " 0.00414353 0.00444003 0.0035076  0.00451988 0.58463204 0.00758196\n",
      " 0.33339804 0.00706699 0.00723199 0.00424098 0.00646606 0.00476833\n",
      " 0.00388849 0.00389947]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00247213 0.00396386 0.00298813 0.00276227 0.00331351 0.00333736\n",
      " 0.0058007  0.00534828 0.003015   0.00307272 0.6091098  0.00565032\n",
      " 0.32097012 0.00513634 0.00413115 0.00320322 0.00457018 0.00359885\n",
      " 0.00460139 0.00295469]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00128257 0.00230365 0.00225407 0.00253308 0.00118735 0.00147428\n",
      " 0.00191965 0.00220258 0.00144245 0.00137745 0.619617   0.00397541\n",
      " 0.34336042 0.00285945 0.00308209 0.00203212 0.00204937 0.00180077\n",
      " 0.00175666 0.00148957]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00215114 0.00191016 0.00180645 0.00188907 0.00207211 0.00197662\n",
      " 0.00181294 0.00178833 0.00202954 0.00265543 0.6207348  0.00388206\n",
      " 0.337104   0.00503553 0.00194892 0.0027251  0.00194818 0.0020958\n",
      " 0.00192571 0.00250813]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00249455 0.00233563 0.00197642 0.00239282 0.00218674 0.0023103\n",
      " 0.00207111 0.00244937 0.00330045 0.00329729 0.58145684 0.00646682\n",
      " 0.36156988 0.00705253 0.0034231  0.00342919 0.00431516 0.00223863\n",
      " 0.00285566 0.00237753]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00158043 0.00228211 0.00164892 0.00136948 0.00203622 0.00189653\n",
      " 0.00262971 0.00268397 0.00175026 0.00199484 0.62374115 0.00387759\n",
      " 0.33858114 0.00306549 0.00205918 0.00183316 0.00146553 0.00147489\n",
      " 0.00170992 0.00231944]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00065798 0.00080279 0.00072402 0.00086486 0.00070958 0.0007404\n",
      " 0.0008383  0.00080796 0.00070948 0.00080162 0.6208157  0.00156724\n",
      " 0.363082   0.00190016 0.00089085 0.00090041 0.00070966 0.00088882\n",
      " 0.00080305 0.00078516]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00212359 0.00219946 0.00201468 0.0022179  0.00210167 0.0032709\n",
      " 0.00189099 0.00192671 0.00161669 0.0027861  0.63102955 0.00351028\n",
      " 0.3262577  0.00304466 0.00266167 0.00276878 0.00158032 0.00166358\n",
      " 0.00195885 0.00337595]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00129281 0.00329351 0.00169289 0.00166321 0.00161927 0.00274316\n",
      " 0.00233881 0.00284549 0.00137207 0.00124059 0.6222966  0.00284134\n",
      " 0.34322488 0.00215931 0.00229659 0.00158194 0.00082493 0.00132791\n",
      " 0.00211523 0.00122952]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00133569 0.00151153 0.00134456 0.00161197 0.00130247 0.00179511\n",
      " 0.00149748 0.00155979 0.00130258 0.00165995 0.6227149  0.00251398\n",
      " 0.34864432 0.00249995 0.00193354 0.00174323 0.00081508 0.00118911\n",
      " 0.00148775 0.00153707]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00099176 0.00130532 0.00085172 0.00116179 0.0011028  0.00119496\n",
      " 0.00137311 0.00128085 0.00124306 0.00119025 0.62318087 0.00199267\n",
      " 0.35410726 0.00211245 0.00134743 0.00158245 0.0006624  0.00094251\n",
      " 0.00134164 0.00103467]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00225422 0.0031604  0.00298586 0.00232346 0.00180768 0.00336943\n",
      " 0.00318276 0.00341585 0.00158368 0.00244022 0.6183649  0.00347486\n",
      " 0.33216625 0.0031885  0.00348027 0.00246193 0.00325789 0.00197657\n",
      " 0.00235825 0.00274707]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00339429 0.00299993 0.00288023 0.0031516  0.0033155  0.00346092\n",
      " 0.00398861 0.00357118 0.00388753 0.00445198 0.56551886 0.00800776\n",
      " 0.35200354 0.00785542 0.00577422 0.00551929 0.0075742  0.00427108\n",
      " 0.00385542 0.00451848]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00098583 0.00081145 0.00091955 0.00081733 0.00085143 0.00096709\n",
      " 0.0009713  0.00097397 0.00093802 0.00123304 0.62010485 0.0023344\n",
      " 0.3586368  0.00245433 0.00129148 0.00112591 0.00107954 0.0013306\n",
      " 0.00090017 0.00127289]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00217014 0.00215307 0.00231175 0.00207108 0.00161029 0.00256307\n",
      " 0.00210173 0.00223122 0.00190601 0.00219948 0.63322645 0.00360461\n",
      " 0.32315034 0.00427718 0.00312172 0.00197805 0.00243862 0.00249664\n",
      " 0.0022238  0.00216473]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0014039  0.00282103 0.00181721 0.00172488 0.0018682  0.00233651\n",
      " 0.00257783 0.00299663 0.00153315 0.00147036 0.61916864 0.0032497\n",
      " 0.34324068 0.0026682  0.00231953 0.00184894 0.00147916 0.00176936\n",
      " 0.00216616 0.00153994]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00277113 0.00241155 0.00239048 0.0025709  0.00254843 0.00295652\n",
      " 0.00287961 0.00299189 0.00274659 0.00401417 0.626122   0.00483264\n",
      " 0.3161223  0.0056379  0.00346234 0.00326778 0.0034273  0.00245996\n",
      " 0.00325092 0.00313557]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [6.3816778e-04 9.4145397e-04 7.8358559e-04 9.0262282e-04 6.0648622e-04\n",
      " 6.9455139e-04 9.0307673e-04 8.3375355e-04 6.2202179e-04 6.7624846e-04\n",
      " 6.2253261e-01 1.4190403e-03 3.6170068e-01 2.1215165e-03 7.4945274e-04\n",
      " 7.3818886e-04 7.1340043e-04 9.1605668e-04 8.0308737e-04 7.0403458e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00120963 0.00135924 0.00142456 0.00115452 0.00116312 0.00136587\n",
      " 0.00148703 0.00156998 0.00112752 0.00153281 0.6213098  0.00301765\n",
      " 0.3503226  0.00389983 0.00123394 0.00120854 0.00133089 0.00155395\n",
      " 0.00138981 0.00133871]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00094282 0.0017083  0.001194   0.00143969 0.00114618 0.00170804\n",
      " 0.00157357 0.00171399 0.00099183 0.00124604 0.6222251  0.0020947\n",
      " 0.3526566  0.00234097 0.00168028 0.00094044 0.00070505 0.00103066\n",
      " 0.00155352 0.00110824]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00167043 0.00483381 0.0030937  0.00276099 0.002569   0.00257588\n",
      " 0.00346141 0.00434872 0.00169859 0.00193063 0.62148005 0.00391436\n",
      " 0.32696968 0.00334252 0.00404936 0.00207895 0.00122205 0.0028903\n",
      " 0.002976   0.00213358]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00223673 0.00302261 0.00193538 0.00195346 0.0023714  0.00325553\n",
      " 0.00266387 0.00272843 0.0025526  0.00286104 0.619333   0.00671146\n",
      " 0.32566607 0.00586303 0.002913   0.00376884 0.00332422 0.00187932\n",
      " 0.00233245 0.00262755]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00130896 0.00166603 0.00197009 0.00101546 0.00129962 0.00131969\n",
      " 0.00182858 0.00208556 0.00159019 0.00164311 0.6272959  0.00647751\n",
      " 0.33269638 0.00388615 0.00197133 0.0023848  0.00417044 0.00244229\n",
      " 0.00127861 0.00166929]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00152582 0.00232887 0.0019475  0.00161588 0.00194393 0.00158048\n",
      " 0.00217257 0.00258256 0.00228879 0.00234987 0.6084193  0.00671898\n",
      " 0.34407216 0.00495084 0.0028306  0.00250469 0.00423254 0.00210523\n",
      " 0.00186735 0.00196206]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00167463 0.00190329 0.0018828  0.00216934 0.0017324  0.0016265\n",
      " 0.00200442 0.00208252 0.0023156  0.00263719 0.5994452  0.00682336\n",
      " 0.34878278 0.00734667 0.00272945 0.00274619 0.00463886 0.00310606\n",
      " 0.00222506 0.0021277 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00147872 0.00133477 0.00113204 0.00118945 0.0011324  0.00139277\n",
      " 0.00149992 0.00133662 0.00133474 0.00122539 0.6219499  0.0030687\n",
      " 0.3484825  0.00352821 0.00128452 0.00220109 0.0022081  0.00141161\n",
      " 0.00128072 0.00152775]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00147366 0.00149245 0.00140272 0.00141823 0.00143802 0.00156387\n",
      " 0.00145621 0.00160558 0.00198702 0.00198671 0.63559556 0.00487159\n",
      " 0.32501948 0.00445561 0.00264636 0.00280344 0.00335704 0.00227489\n",
      " 0.00138727 0.00176429]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00183385 0.00142264 0.00210161 0.00147676 0.0014893  0.00165379\n",
      " 0.00162165 0.00185631 0.00167081 0.00244825 0.6283418  0.00539993\n",
      " 0.32642245 0.00582704 0.00275241 0.00218482 0.00434834 0.00313833\n",
      " 0.00138559 0.00262434]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.3177017e-04 6.2950281e-04 5.6642760e-04 5.9912261e-04 5.7604094e-04\n",
      " 5.7526585e-04 7.1694137e-04 6.4350246e-04 5.3440896e-04 6.1934168e-04\n",
      " 6.2246394e-01 1.1874151e-03 3.6478236e-01 1.6327074e-03 7.1730686e-04\n",
      " 6.1707618e-04 4.6009428e-04 8.2640321e-04 6.9063774e-04 6.2973850e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00091852 0.00148597 0.00114754 0.00129397 0.00112788 0.00138112\n",
      " 0.00141572 0.00148833 0.00097719 0.00107454 0.6181414  0.00222207\n",
      " 0.3567918  0.00269771 0.0014727  0.00130949 0.00135094 0.00129533\n",
      " 0.00130267 0.00110507]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.001398   0.00147379 0.00151433 0.0013021  0.00147311 0.00145587\n",
      " 0.00147709 0.00163477 0.00142556 0.00231839 0.61664605 0.00383787\n",
      " 0.3485729  0.00490639 0.00162992 0.00172201 0.00193868 0.00199559\n",
      " 0.00164591 0.0016317 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00123038 0.00192528 0.00183176 0.00171935 0.00169227 0.00173627\n",
      " 0.00225822 0.00241551 0.00115377 0.00157513 0.62037706 0.00292564\n",
      " 0.34586197 0.00265701 0.00230194 0.00152269 0.00140209 0.00180619\n",
      " 0.00164397 0.00196352]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00250573 0.0020777  0.00203051 0.00218304 0.00228857 0.00223375\n",
      " 0.00204515 0.00203456 0.00259141 0.00330971 0.6213497  0.00458084\n",
      " 0.3267953  0.00665335 0.00257572 0.00320523 0.0035054  0.00266486\n",
      " 0.00238154 0.00298792]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00179481 0.00261986 0.00161919 0.00125308 0.00204182 0.00279504\n",
      " 0.00243493 0.00247282 0.00209216 0.00203648 0.6136825  0.00619399\n",
      " 0.3401984  0.0038525  0.00192857 0.00349736 0.0033387  0.00216759\n",
      " 0.00175488 0.00222532]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0012673  0.00146853 0.00155475 0.00157112 0.00140045 0.00161457\n",
      " 0.00160153 0.00177171 0.00123814 0.00172069 0.62130487 0.00275539\n",
      " 0.349104   0.00244275 0.00213934 0.0016144  0.00100275 0.00133395\n",
      " 0.00152226 0.00157154]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00077608 0.00095847 0.00084917 0.00100878 0.00073994 0.00098045\n",
      " 0.00102393 0.00101036 0.00080826 0.00113229 0.62310755 0.00175818\n",
      " 0.35827515 0.00215954 0.00097491 0.00087795 0.0007421  0.00085939\n",
      " 0.00110897 0.00084852]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [4.9811765e-04 7.0672896e-04 5.8663148e-04 6.5711210e-04 5.9910456e-04\n",
      " 6.3365925e-04 6.7244790e-04 7.4776734e-04 5.5513787e-04 6.7846267e-04\n",
      " 6.2298417e-01 1.5972203e-03 3.6363548e-01 1.6580471e-03 7.2723028e-04\n",
      " 6.1581394e-04 5.7563593e-04 6.3089712e-04 6.0261623e-04 6.3768413e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00126522 0.00126867 0.00121254 0.00178374 0.00108476 0.00168616\n",
      " 0.0012033  0.00125922 0.00126591 0.00197769 0.627633   0.00307033\n",
      " 0.34265786 0.0030505  0.00175008 0.00188298 0.00194841 0.0009839\n",
      " 0.00132207 0.00169366]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00240283 0.00342617 0.00256323 0.00225774 0.00232524 0.00305507\n",
      " 0.00426726 0.00442155 0.00283893 0.00285993 0.60629493 0.00665917\n",
      " 0.3267519  0.00527302 0.0044793  0.00326564 0.00668137 0.0042089\n",
      " 0.00289681 0.00307102]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00116247 0.00142    0.00123306 0.0011394  0.00121738 0.00126201\n",
      " 0.0013343  0.00142281 0.00132755 0.00158053 0.6227019  0.00318137\n",
      " 0.34883195 0.00382624 0.00132362 0.00169673 0.00112945 0.00153516\n",
      " 0.00137154 0.0013025 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00064902 0.00097264 0.00072945 0.00067966 0.00090506 0.00076451\n",
      " 0.00093095 0.00097002 0.00078428 0.00082284 0.62288606 0.00215208\n",
      " 0.3595317  0.00208434 0.00090252 0.00091351 0.00064989 0.00103884\n",
      " 0.00085414 0.00077851]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0017096  0.00274227 0.00183105 0.00178764 0.00140288 0.00249247\n",
      " 0.00199871 0.00255878 0.00162675 0.00188678 0.6210695  0.00320689\n",
      " 0.34278315 0.0026864  0.0027987  0.0016859  0.00103704 0.00129716\n",
      " 0.00192335 0.00147495]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00081132 0.00087771 0.00076529 0.00088696 0.00070734 0.00096792\n",
      " 0.00092732 0.00086635 0.00071409 0.00079956 0.6228705  0.0016871\n",
      " 0.3596627  0.00193035 0.00087151 0.00125253 0.00076955 0.00088959\n",
      " 0.00076536 0.00097692]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00085442 0.00109285 0.00102887 0.00110943 0.00091376 0.00100716\n",
      " 0.00099436 0.00112372 0.00099722 0.00117579 0.6208224  0.00296721\n",
      " 0.355638   0.0032406  0.00119848 0.00130101 0.00147629 0.00105119\n",
      " 0.00099802 0.00100918]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0012395  0.00136041 0.00132741 0.00146761 0.00117565 0.0016156\n",
      " 0.00134387 0.00141727 0.00122446 0.00180236 0.6202673  0.00322603\n",
      " 0.35024542 0.00337992 0.00155415 0.00166674 0.0015757  0.00130135\n",
      " 0.00135239 0.00145687]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00105532 0.00130081 0.00125439 0.00105259 0.00114914 0.00118075\n",
      " 0.00155853 0.00144493 0.00103585 0.00137387 0.6209811  0.00327781\n",
      " 0.35145822 0.00341155 0.00121287 0.00142663 0.00175305 0.00138429\n",
      " 0.00118211 0.00150619]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00101778 0.00131145 0.0010875  0.001126   0.00108199 0.00114991\n",
      " 0.00120199 0.00132954 0.00122615 0.00129284 0.62116915 0.00304714\n",
      " 0.3530296  0.00320655 0.00124593 0.00169801 0.00141909 0.00107559\n",
      " 0.00111399 0.00116985]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00149969 0.00121278 0.00136328 0.00154988 0.00124153 0.00179588\n",
      " 0.00124378 0.0013097  0.00136479 0.00225983 0.63251364 0.00315383\n",
      " 0.33655983 0.00299291 0.00186633 0.00206298 0.00169654 0.00107472\n",
      " 0.00121189 0.00202625]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00120725 0.00151871 0.00143836 0.0019465  0.00125491 0.00132191\n",
      " 0.00155314 0.00211515 0.00144473 0.00161653 0.6160618  0.00336796\n",
      " 0.35076725 0.00448964 0.00230372 0.00150666 0.00178067 0.0013603\n",
      " 0.00175718 0.0011877 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00160905 0.00130017 0.00154391 0.00230021 0.00109792 0.00197881\n",
      " 0.0014852  0.00139146 0.00105636 0.00185708 0.62100065 0.00219386\n",
      " 0.34817708 0.00327103 0.00182074 0.00151808 0.00130614 0.00141917\n",
      " 0.00149371 0.0021794 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00106762 0.00148716 0.00115053 0.00139599 0.00122141 0.00158154\n",
      " 0.00128141 0.00144203 0.00116969 0.0012917  0.6224363  0.00304868\n",
      " 0.35079265 0.00290616 0.00157574 0.00151638 0.00104589 0.00114679\n",
      " 0.00133867 0.00110364]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0014727  0.00201089 0.00180149 0.00135351 0.00181811 0.00183918\n",
      " 0.00219543 0.0020145  0.00142944 0.00177044 0.6231364  0.00341903\n",
      " 0.3419316  0.00289681 0.00181321 0.00204788 0.00130368 0.00192582\n",
      " 0.00170413 0.00211581]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00166344 0.0033982  0.00218878 0.00177943 0.00175632 0.00414012\n",
      " 0.00270149 0.00331161 0.0016046  0.00187752 0.6226121  0.00426969\n",
      " 0.33425373 0.00236608 0.00282891 0.00236633 0.00164055 0.00141686\n",
      " 0.00209419 0.00173005]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00187639 0.0022569  0.00203223 0.00191036 0.00225845 0.00238098\n",
      " 0.0024359  0.00244554 0.0020999  0.00257234 0.60576093 0.00562217\n",
      " 0.34316656 0.00588561 0.00302833 0.00278704 0.00414547 0.00242041\n",
      " 0.0021814  0.00273313]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00130246 0.00175702 0.00164775 0.00169066 0.00122657 0.00160051\n",
      " 0.00197281 0.00197772 0.00124444 0.00148621 0.61138684 0.00334172\n",
      " 0.35457543 0.00356578 0.00191788 0.00209798 0.00243178 0.00162999\n",
      " 0.00158223 0.00156422]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00187274 0.00251573 0.00211141 0.00244086 0.00162981 0.00212709\n",
      " 0.00268864 0.00241115 0.0017243  0.00164536 0.6181897  0.00275833\n",
      " 0.33940762 0.00317498 0.00327893 0.00257661 0.00155598 0.00328545\n",
      " 0.00212647 0.00247882]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00143581 0.00138305 0.00144366 0.00135316 0.00134942 0.00161686\n",
      " 0.00144251 0.00153513 0.00155494 0.00193694 0.62029254 0.00418493\n",
      " 0.3454889  0.00385848 0.00187263 0.00212962 0.00219233 0.00169529\n",
      " 0.00144649 0.00178733]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00070129 0.0010583  0.00085423 0.00093328 0.00074129 0.0008835\n",
      " 0.00092473 0.00106935 0.00079628 0.00087847 0.62084234 0.00193612\n",
      " 0.3605566  0.00225539 0.00104501 0.00094717 0.00097061 0.00088816\n",
      " 0.00093478 0.00078305]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00144606 0.00163045 0.00162245 0.00186213 0.00163178 0.00171899\n",
      " 0.00165024 0.00177879 0.00180977 0.00206779 0.63372827 0.00435152\n",
      " 0.32834712 0.00328039 0.00306607 0.00254009 0.0025202  0.00144736\n",
      " 0.00155544 0.00194511]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00132641 0.00130016 0.00126703 0.00110715 0.00130347 0.00154413\n",
      " 0.00164542 0.00163448 0.00132766 0.00150248 0.6263107  0.00387305\n",
      " 0.34216914 0.0027537  0.0016703  0.00238949 0.00266754 0.00116468\n",
      " 0.00110435 0.00193866]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00114837 0.00155251 0.00107919 0.00132341 0.00118028 0.00135311\n",
      " 0.00122789 0.00126816 0.00137354 0.00120305 0.6188885  0.00385285\n",
      " 0.3514433  0.00294666 0.00160833 0.00238694 0.00242835 0.00123286\n",
      " 0.00105309 0.00144963]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00079219 0.00118126 0.00105054 0.00080646 0.00111127 0.0009496\n",
      " 0.00109714 0.00108443 0.00090893 0.0009434  0.6225801  0.00290483\n",
      " 0.3558976  0.00209161 0.00105072 0.00152889 0.00102197 0.00104674\n",
      " 0.00075375 0.00119855]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00260189 0.00424737 0.00307253 0.00332403 0.00313956 0.00333427\n",
      " 0.00398848 0.00441244 0.00332387 0.00358346 0.59153116 0.00635785\n",
      " 0.33539313 0.00784345 0.00485572 0.00400944 0.00391813 0.00404308\n",
      " 0.00431567 0.00270449]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00175072 0.00251124 0.00184221 0.0023954  0.00190229 0.00197974\n",
      " 0.00217806 0.00224352 0.00175404 0.00209059 0.6213795  0.00272927\n",
      " 0.33872294 0.00421091 0.00232406 0.00174799 0.00108367 0.00248259\n",
      " 0.00295298 0.00171824]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00084701 0.00089553 0.00078941 0.00071923 0.00089426 0.00094634\n",
      " 0.00114286 0.0010049  0.00089045 0.00085674 0.6225809  0.00223407\n",
      " 0.3581758  0.00204463 0.00089619 0.00132364 0.00089459 0.0009825\n",
      " 0.0007757  0.00110531]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00209505 0.00204512 0.00225354 0.00203858 0.00197545 0.00253681\n",
      " 0.00188766 0.00199994 0.00178981 0.00283845 0.6199168  0.00449865\n",
      " 0.33509693 0.00501304 0.00232169 0.00245232 0.00203658 0.00242771\n",
      " 0.00210627 0.00266963]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00155792 0.00163847 0.00152106 0.00296492 0.00134278 0.00173232\n",
      " 0.00159747 0.00162667 0.00168252 0.00172196 0.6189655  0.00242489\n",
      " 0.3445257  0.00403688 0.00300348 0.00239301 0.00143611 0.00208244\n",
      " 0.00230838 0.00143751]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00213682 0.00214999 0.00206688 0.00182668 0.00186433 0.0021403\n",
      " 0.0029305  0.00238217 0.00300903 0.00278296 0.5889583  0.00598029\n",
      " 0.3514136  0.00701238 0.00285046 0.00389045 0.00690241 0.00430497\n",
      " 0.00283413 0.00256331]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0017722  0.00209715 0.00176109 0.00209502 0.00182661 0.0016589\n",
      " 0.0021268  0.00228014 0.0026265  0.00271144 0.59490246 0.00565601\n",
      " 0.35628983 0.00683496 0.0027751  0.00241273 0.00337332 0.00239239\n",
      " 0.0025156  0.00189174]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00070742 0.00134681 0.00128487 0.00132464 0.00102927 0.00103487\n",
      " 0.00123319 0.00146933 0.00071726 0.00103397 0.62248313 0.00189063\n",
      " 0.35640705 0.00228187 0.00116283 0.00078398 0.00072515 0.00091945\n",
      " 0.00130521 0.00085909]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00156585 0.00212089 0.00147925 0.00186775 0.00191299 0.00146433\n",
      " 0.00220586 0.00170636 0.00216638 0.00170714 0.6237121  0.003172\n",
      " 0.33556646 0.00329699 0.00243082 0.00414746 0.00208722 0.00289277\n",
      " 0.00251201 0.00198532]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00305316 0.00222423 0.00262706 0.00366427 0.00280308 0.00257364\n",
      " 0.00268282 0.00260346 0.00313924 0.00440074 0.56934696 0.0054299\n",
      " 0.35970402 0.01071792 0.00399812 0.00310078 0.00626939 0.00408532\n",
      " 0.00415185 0.00342405]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00158666 0.00269758 0.00189546 0.00148092 0.00202445 0.0022533\n",
      " 0.00226885 0.00269516 0.00194808 0.00217984 0.62335014 0.00557857\n",
      " 0.3338154  0.00371584 0.0023279  0.0023399  0.00217942 0.00175191\n",
      " 0.00210274 0.00180787]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00128874 0.0028436  0.00215763 0.00260208 0.00142871 0.00176185\n",
      " 0.00228216 0.00254395 0.00156885 0.00154166 0.61631584 0.00413637\n",
      " 0.34256107 0.00319445 0.0032728  0.00239199 0.0021513  0.00229939\n",
      " 0.00193969 0.00171784]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00261279 0.00381676 0.00232181 0.00246121 0.00270758 0.00354147\n",
      " 0.00310621 0.00337187 0.00332641 0.00293693 0.60002923 0.00685899\n",
      " 0.336294   0.00571793 0.00371223 0.00545779 0.00321863 0.00294388\n",
      " 0.00294621 0.00261807]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00106293 0.00152086 0.00130769 0.00136371 0.00111558 0.00176998\n",
      " 0.00139739 0.0014883  0.00108089 0.00142601 0.6229036  0.00281203\n",
      " 0.35101458 0.00209888 0.00174038 0.00131907 0.00104621 0.00093079\n",
      " 0.00125883 0.00134226]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00090318 0.00181994 0.00173787 0.00136458 0.00126699 0.00132423\n",
      " 0.00166916 0.00227029 0.00094303 0.0013693  0.62145734 0.00272404\n",
      " 0.35095382 0.00289482 0.00143755 0.00109517 0.00087472 0.0013131\n",
      " 0.00155602 0.00102481]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00198107 0.00292896 0.00295072 0.00170109 0.00202557 0.00299355\n",
      " 0.00348574 0.00369141 0.00160748 0.00303294 0.61517346 0.00435628\n",
      " 0.3354929  0.00357863 0.00267857 0.00166334 0.00323358 0.00217446\n",
      " 0.00219645 0.00305386]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00064565 0.00091402 0.00072242 0.00086541 0.00066763 0.00080395\n",
      " 0.0009039  0.00087876 0.0006683  0.00074344 0.622049   0.00153754\n",
      " 0.36196765 0.00192631 0.0008248  0.00077854 0.00071321 0.00076978\n",
      " 0.00094878 0.00067089]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00125825 0.00141099 0.00149563 0.00139474 0.00129248 0.00183616\n",
      " 0.00151405 0.00152877 0.00118077 0.00180967 0.6409077  0.00297521\n",
      " 0.3285507  0.00239915 0.00204549 0.00205526 0.00160347 0.00143775\n",
      " 0.00143336 0.00187044]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00077958 0.00142229 0.0009186  0.00106721 0.00093352 0.00123018\n",
      " 0.0014615  0.00134387 0.00077408 0.00087408 0.6215355  0.00166938\n",
      " 0.3582698  0.00185901 0.00097585 0.00087264 0.00104681 0.00083265\n",
      " 0.00127024 0.00086321]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00345494 0.00321677 0.00640111 0.00387358 0.00267803 0.00540905\n",
      " 0.00354755 0.0044367  0.00338014 0.00549338 0.57155025 0.00995071\n",
      " 0.3237328  0.01363805 0.00732135 0.00485204 0.01115918 0.00842494\n",
      " 0.0035252  0.00395421]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00136184 0.00178923 0.00146634 0.00143894 0.00158561 0.00183822\n",
      " 0.00170642 0.00191118 0.00136646 0.00158743 0.61969113 0.00322551\n",
      " 0.3482006  0.00268005 0.00225848 0.00182161 0.00138353 0.00140775\n",
      " 0.00142487 0.00185485]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00077136 0.0010604  0.00094595 0.00082223 0.00113948 0.00096992\n",
      " 0.00103039 0.001077   0.00076506 0.00105795 0.6215074  0.00253507\n",
      " 0.35767508 0.00242892 0.00101311 0.00107055 0.00095485 0.00106801\n",
      " 0.00081975 0.00128753]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00216077 0.00342359 0.00286673 0.00354569 0.0021732  0.00295691\n",
      " 0.00249378 0.00314199 0.00205978 0.00248263 0.6104007  0.00501878\n",
      " 0.33303922 0.00634853 0.00378587 0.00334157 0.00276928 0.00315947\n",
      " 0.00260502 0.0022265 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00206992 0.00220723 0.00210288 0.00232955 0.00232909 0.00217545\n",
      " 0.00224903 0.00234464 0.00237223 0.00289983 0.6414035  0.00477407\n",
      " 0.30839893 0.0048009  0.00348893 0.00308587 0.00335888 0.0023822\n",
      " 0.00241082 0.00281608]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00150203 0.0018042  0.00161149 0.00205799 0.00167474 0.00181996\n",
      " 0.00181025 0.00181545 0.00144094 0.00178726 0.62092984 0.00246085\n",
      " 0.34515858 0.00275028 0.0024105  0.00183687 0.00096605 0.00205963\n",
      " 0.0023622  0.00174083]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00268092 0.00242563 0.00310533 0.0025421  0.00240569 0.00396257\n",
      " 0.00324897 0.00327663 0.00216317 0.00304333 0.6136018  0.00556383\n",
      " 0.32778493 0.00399993 0.00439631 0.0032697  0.00291594 0.0030702\n",
      " 0.00234302 0.0042    ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00110427 0.00185797 0.00168754 0.00155234 0.00140398 0.00157098\n",
      " 0.00149617 0.00200406 0.00125053 0.00157938 0.6169464  0.00305556\n",
      " 0.35230604 0.00339039 0.00166932 0.00148063 0.00150135 0.00132345\n",
      " 0.00161264 0.00120699]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00143683 0.00176035 0.0020458  0.00227934 0.00154461 0.00200234\n",
      " 0.00162149 0.0021431  0.00152272 0.00226161 0.62956786 0.00484474\n",
      " 0.32905757 0.00526828 0.00289439 0.00210608 0.00244717 0.00175421\n",
      " 0.0016641  0.00177735]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00234558 0.00325047 0.00238988 0.00236931 0.00280939 0.00235349\n",
      " 0.0023186  0.00272472 0.00331652 0.00329845 0.59679294 0.00727454\n",
      " 0.34097636 0.00817952 0.00290068 0.00418702 0.00371216 0.00319137\n",
      " 0.00320282 0.0024062 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00209455 0.00210647 0.00177073 0.00219574 0.00172607 0.00257858\n",
      " 0.00326812 0.00240006 0.00170783 0.00206598 0.62112594 0.00324515\n",
      " 0.3314753  0.00363374 0.00216446 0.00353834 0.00509334 0.00237693\n",
      " 0.00267875 0.00275391]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00359336 0.00483967 0.0033096  0.00416382 0.00349529 0.00527004\n",
      " 0.00491149 0.00501798 0.00310944 0.00347743 0.5964235  0.00644843\n",
      " 0.32111704 0.00567421 0.00519217 0.00601194 0.0050411  0.00460118\n",
      " 0.0035775  0.00472476]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00077577 0.00098044 0.00085181 0.00082396 0.00080617 0.00082374\n",
      " 0.00094673 0.00100362 0.0008483  0.00094544 0.6226059  0.00245171\n",
      " 0.35783482 0.0025508  0.00092723 0.00101504 0.0009315  0.00104594\n",
      " 0.00091526 0.00091574]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00072334 0.00099601 0.00085218 0.00093829 0.00075687 0.00110278\n",
      " 0.00088902 0.0010722  0.00079696 0.00107133 0.62220937 0.00214391\n",
      " 0.3593672  0.00194061 0.00114074 0.0008601  0.00062259 0.00080748\n",
      " 0.00093366 0.00077544]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00075915 0.00158116 0.0012294  0.00120049 0.00115113 0.0010787\n",
      " 0.00142018 0.00174128 0.00079495 0.00103023 0.62137425 0.00217128\n",
      " 0.35597527 0.00214634 0.00132708 0.00091258 0.0009075  0.0010082\n",
      " 0.00118208 0.0010087 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00141516 0.00212591 0.00159377 0.00187484 0.00121098 0.00231214\n",
      " 0.00172375 0.00196714 0.00127398 0.00176922 0.6177337  0.00360149\n",
      " 0.34866443 0.00310154 0.0017242  0.00187271 0.00172964 0.00125487\n",
      " 0.00156614 0.0014843 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00067303 0.00086344 0.00092291 0.00101551 0.00078593 0.00083495\n",
      " 0.00085496 0.00093536 0.00069413 0.00097916 0.6218977  0.00187949\n",
      " 0.36017308 0.00237098 0.00101116 0.00081515 0.00066032 0.00089457\n",
      " 0.00083878 0.00089935]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00197163 0.0027817  0.00187002 0.00200483 0.00218591 0.00247986\n",
      " 0.00297163 0.00314008 0.00275423 0.00290834 0.6106032  0.00630915\n",
      " 0.33570272 0.00501823 0.00319702 0.00290277 0.00432897 0.00182729\n",
      " 0.00280679 0.00223563]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00176678 0.00198857 0.00185133 0.00181672 0.00203809 0.00217325\n",
      " 0.00205942 0.00207312 0.00178685 0.00266002 0.6186728  0.0037208\n",
      " 0.34230858 0.00328374 0.00232109 0.00203731 0.00159828 0.00143489\n",
      " 0.00208734 0.00232101]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0009163  0.00139837 0.00127694 0.00110344 0.00122258 0.00123312\n",
      " 0.0012607  0.00160908 0.00099587 0.00127686 0.62129086 0.00426561\n",
      " 0.35153103 0.0031062  0.00128416 0.00126868 0.00159366 0.00112048\n",
      " 0.00105752 0.00118857]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00147204 0.00177707 0.00157027 0.00196779 0.00160574 0.00137829\n",
      " 0.00174705 0.0018457  0.00218131 0.00195836 0.60545754 0.00503116\n",
      " 0.35122845 0.00621441 0.00293592 0.00269211 0.00264517 0.00272285\n",
      " 0.00191513 0.00165361]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00259746 0.00288    0.00266604 0.00272506 0.00280017 0.0030036\n",
      " 0.00336662 0.00369473 0.00373207 0.00403316 0.56885004 0.01080967\n",
      " 0.34612718 0.01379821 0.00456667 0.00519494 0.00917513 0.00398365\n",
      " 0.00286596 0.00312963]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00186612 0.00209836 0.00241638 0.00287072 0.00181357 0.00249544\n",
      " 0.001982   0.00221517 0.00164635 0.00220558 0.61562693 0.00390867\n",
      " 0.33787024 0.0056464  0.0029248  0.00221081 0.00242531 0.0034182\n",
      " 0.00224676 0.0021122 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00087408 0.00114225 0.00097307 0.00097887 0.00091877 0.00121847\n",
      " 0.00105758 0.00118718 0.00100895 0.00119154 0.6182338  0.00288896\n",
      " 0.35909647 0.00227803 0.00124468 0.0012051  0.00159024 0.00090733\n",
      " 0.0010053  0.00099934]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0010382  0.00134675 0.00142755 0.00128409 0.00200595 0.00110563\n",
      " 0.00192082 0.00209066 0.00143768 0.00199981 0.61790615 0.00494517\n",
      " 0.34417254 0.00427367 0.00282405 0.00162598 0.00294503 0.00232149\n",
      " 0.00152619 0.00180254]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00312466 0.00349241 0.00400388 0.00352493 0.00320515 0.0030872\n",
      " 0.0042181  0.00448437 0.00418341 0.0052595  0.57505643 0.01275068\n",
      " 0.32338232 0.01428865 0.00549676 0.00500678 0.0112466  0.0058862\n",
      " 0.00359211 0.0047099 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00123463 0.00110494 0.00127065 0.00137093 0.00127909 0.00121194\n",
      " 0.0012334  0.00128023 0.00139755 0.00173775 0.60558474 0.00369761\n",
      " 0.35980263 0.00497153 0.00236118 0.00190874 0.00341969 0.00213737\n",
      " 0.00133637 0.00165905]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00083293 0.00153898 0.00119048 0.00136908 0.00108537 0.00114231\n",
      " 0.0011466  0.00152149 0.00092144 0.00107703 0.6208538  0.00229991\n",
      " 0.35562718 0.00262349 0.00148256 0.00109053 0.0008805  0.00114983\n",
      " 0.00133705 0.00082943]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00078861 0.00090756 0.00113797 0.00091693 0.00104722 0.00104779\n",
      " 0.00115317 0.00115317 0.0007883  0.00113573 0.6230055  0.00258489\n",
      " 0.35567817 0.00207142 0.00125354 0.0010008  0.00099476 0.00102805\n",
      " 0.00088849 0.00141788]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00117851 0.00201838 0.00149644 0.00184317 0.0012118  0.00152094\n",
      " 0.00191963 0.00226103 0.00146042 0.0015851  0.61984724 0.00331668\n",
      " 0.3469345  0.00367695 0.0020089  0.00152801 0.00161192 0.00134168\n",
      " 0.00190097 0.00133774]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00073462 0.00104174 0.00084824 0.00093416 0.00077897 0.00090844\n",
      " 0.0009786  0.00108038 0.00080764 0.00102878 0.6211826  0.00196296\n",
      " 0.3596111  0.00246899 0.00103846 0.00095234 0.0008486  0.0009411\n",
      " 0.0009309  0.00092134]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00076904 0.00144646 0.00097441 0.00122889 0.00088185 0.00122817\n",
      " 0.00106477 0.00133675 0.00076558 0.0007949  0.6216177  0.00182126\n",
      " 0.358496   0.00182377 0.00144132 0.00109305 0.00067038 0.00082079\n",
      " 0.00089446 0.00083052]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00191791 0.00408817 0.002813   0.00223084 0.00326694 0.00252122\n",
      " 0.00556198 0.00593283 0.00265649 0.00307966 0.61635697 0.00605879\n",
      " 0.3150648  0.00551192 0.0047627  0.0020652  0.00570181 0.00347574\n",
      " 0.00414859 0.00278447]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00126065 0.00151364 0.00112999 0.00124298 0.00113689 0.00163175\n",
      " 0.00120028 0.00133163 0.00128492 0.00145467 0.6213174  0.00319453\n",
      " 0.35130024 0.00302803 0.00134043 0.00172893 0.00125319 0.00119171\n",
      " 0.00123538 0.00122274]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00128254 0.0022496  0.00140343 0.00191405 0.00150895 0.00200283\n",
      " 0.002256   0.00190335 0.00118543 0.00115466 0.6208638  0.00216181\n",
      " 0.3481123  0.0024576  0.00178661 0.00158882 0.00114381 0.00150395\n",
      " 0.00205434 0.0014662 ]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00154615 0.00181902 0.00153211 0.00135875 0.00162105 0.0017465\n",
      " 0.00207168 0.00190664 0.00180546 0.00148343 0.62396806 0.00458576\n",
      " 0.33696455 0.00382088 0.00197975 0.00315917 0.00267898 0.00248838\n",
      " 0.00155465 0.00190903]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00246005 0.00271227 0.00261415 0.00274083 0.0031848  0.00339293\n",
      " 0.0021728  0.0024285  0.00262732 0.00326498 0.6177715  0.00716255\n",
      " 0.32268226 0.00509689 0.00420249 0.00434967 0.00321363 0.00234565\n",
      " 0.00240204 0.0031747 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00381638 0.00486367 0.00631253 0.00398847 0.00414545 0.00597713\n",
      " 0.00505976 0.00590053 0.00299983 0.00487329 0.58759356 0.00966114\n",
      " 0.30731744 0.00738749 0.00767643 0.00551036 0.00874231 0.00760971\n",
      " 0.00354149 0.00702305]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [6.1000383e-04 7.8158599e-04 5.9742620e-04 6.0094451e-04 6.8528845e-04\n",
      " 6.7940634e-04 7.4023451e-04 7.9757528e-04 6.6772616e-04 7.3347543e-04\n",
      " 6.2299031e-01 1.8908381e-03 3.6212054e-01 1.9329976e-03 6.6974969e-04\n",
      " 7.7689340e-04 6.7378883e-04 6.9450005e-04 6.2129187e-04 7.3539338e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00084906 0.0010851  0.00094164 0.00114567 0.00092505 0.00098511\n",
      " 0.00099756 0.00105204 0.00094032 0.00111518 0.6163762  0.00249957\n",
      " 0.36029625 0.0028832  0.00124032 0.00145259 0.00191538 0.00115071\n",
      " 0.00103092 0.00111814]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.3217955e-04 7.3904824e-04 7.3470676e-04 6.6703546e-04 6.6944980e-04\n",
      " 6.8365107e-04 8.1268704e-04 8.5614249e-04 5.4771442e-04 6.9661508e-04\n",
      " 6.2234837e-01 1.5951651e-03 3.6310059e-01 1.6353159e-03 8.2349480e-04\n",
      " 6.8439101e-04 5.6303164e-04 8.6782785e-04 7.0517231e-04 7.3740771e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [7.1058417e-04 8.6204999e-04 7.7892287e-04 1.0562380e-03 6.7779934e-04\n",
      " 9.2958007e-04 8.1915257e-04 8.6173607e-04 7.3206058e-04 8.7577890e-04\n",
      " 6.2223315e-01 1.5468418e-03 3.6098808e-01 1.9608459e-03 9.6643216e-04\n",
      " 9.0204942e-04 6.0398388e-04 8.0008770e-04 1.0186157e-03 6.7601574e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00251451 0.00268729 0.00267589 0.00288745 0.00263206 0.00241191\n",
      " 0.00239403 0.00265373 0.00336602 0.00369923 0.58499646 0.00692036\n",
      " 0.34932047 0.00984554 0.00387833 0.00373919 0.00479147 0.00309807\n",
      " 0.0029291  0.00255895]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00083308 0.00125739 0.00094442 0.00100531 0.00082858 0.00102004\n",
      " 0.00111046 0.00116219 0.00092545 0.00103926 0.6209259  0.00226569\n",
      " 0.35808617 0.0024255  0.00107911 0.00113446 0.00105037 0.00099855\n",
      " 0.00100957 0.00089849]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0020922  0.0035834  0.00303685 0.00372882 0.00222019 0.00210666\n",
      " 0.00296996 0.00327334 0.00296629 0.00298878 0.5864668  0.00758549\n",
      " 0.34556302 0.00893054 0.00442483 0.00381881 0.00522171 0.00359472\n",
      " 0.00305865 0.00236893]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00279469 0.00344152 0.00264955 0.00332365 0.00272088 0.00246679\n",
      " 0.0027979  0.00317365 0.0046101  0.00368049 0.56442225 0.00858907\n",
      " 0.35675663 0.01045571 0.00532632 0.00648875 0.00510907 0.00486118\n",
      " 0.00357763 0.00275422]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0018215  0.00228091 0.00222585 0.00186032 0.001893   0.00242059\n",
      " 0.00233718 0.00259307 0.00167935 0.00234703 0.62131613 0.00512188\n",
      " 0.33402568 0.00358934 0.00292253 0.00307346 0.00224141 0.00184385\n",
      " 0.00183477 0.00257214]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [4.0535390e-04 5.0058379e-04 4.6497679e-04 4.6604045e-04 4.3283211e-04\n",
      " 4.5321480e-04 4.9942889e-04 5.3178816e-04 4.4513826e-04 4.8034443e-04\n",
      " 6.2279558e-01 1.1466595e-03 3.6696717e-01 1.4286077e-03 5.5025640e-04\n",
      " 4.9821148e-04 4.1086401e-04 5.8035803e-04 5.1362207e-04 4.2898857e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00415913 0.00322369 0.00409052 0.00364121 0.00273807 0.00506351\n",
      " 0.00443556 0.00367349 0.00367332 0.00423856 0.60049397 0.00674602\n",
      " 0.31475955 0.00555798 0.00577306 0.00649885 0.00671954 0.00517375\n",
      " 0.00344541 0.00589484]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00197072 0.00207824 0.00253073 0.00288256 0.00165684 0.0019301\n",
      " 0.00244429 0.00222714 0.00155399 0.0023114  0.6135943  0.00316891\n",
      " 0.34229568 0.00487745 0.00242085 0.00212222 0.00217719 0.00277686\n",
      " 0.00248404 0.00249649]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0009351  0.00114145 0.00100013 0.00110812 0.00086166 0.00124588\n",
      " 0.00104452 0.00111257 0.0009529  0.00131815 0.621897   0.00250971\n",
      " 0.3555153  0.00284846 0.00113833 0.00115983 0.00107051 0.00096517\n",
      " 0.0010288  0.00114641]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00096206 0.00224194 0.00129432 0.00139446 0.0011343  0.00165427\n",
      " 0.00184148 0.0021874  0.00087924 0.00103848 0.6221081  0.001957\n",
      " 0.35268277 0.00177902 0.00167716 0.00099083 0.00090857 0.00082259\n",
      " 0.0013518  0.0010942 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00125678 0.00199766 0.00154285 0.00157494 0.00139492 0.00148615\n",
      " 0.00216271 0.00238817 0.00150624 0.00180832 0.61707866 0.00371555\n",
      " 0.34697938 0.00384772 0.00200863 0.00214739 0.00230168 0.00161989\n",
      " 0.00172788 0.00145453]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00070907 0.00094895 0.0008438  0.00095104 0.00069172 0.00093149\n",
      " 0.00090661 0.0009577  0.00071565 0.00088483 0.62125015 0.00169543\n",
      " 0.36109582 0.00220558 0.00096965 0.00088432 0.00081176 0.00089282\n",
      " 0.00085062 0.00080294]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00199911 0.00254877 0.00198696 0.00206844 0.00209556 0.00231482\n",
      " 0.00213413 0.002539   0.00266204 0.0021967  0.6199796  0.00712194\n",
      " 0.32621014 0.00547638 0.00324804 0.00446918 0.00433325 0.00252411\n",
      " 0.00189009 0.00220166]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00764781 0.00902794 0.01103904 0.01238225 0.00557578 0.01033526\n",
      " 0.00721627 0.00827099 0.00365458 0.00588253 0.51745033 0.00910459\n",
      " 0.33475465 0.00925123 0.00878505 0.01056512 0.00745589 0.00614472\n",
      " 0.00646286 0.00899318]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00337137 0.00300656 0.00322203 0.00339962 0.00375685 0.00276997\n",
      " 0.00334525 0.00367049 0.00446895 0.00512755 0.5541346  0.01073167\n",
      " 0.3551724  0.01389008 0.00482834 0.00457544 0.0075031  0.00488829\n",
      " 0.00402405 0.00411342]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0024688  0.00228925 0.00201804 0.00241583 0.0012808  0.0039451\n",
      " 0.00239544 0.00200065 0.00164661 0.00167979 0.62174857 0.00243102\n",
      " 0.33719558 0.00261249 0.00238098 0.00323804 0.00202145 0.00175086\n",
      " 0.00245136 0.00202936]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0017493  0.00163818 0.00152286 0.0013333  0.00195396 0.00215205\n",
      " 0.00200622 0.00182868 0.00132114 0.00170875 0.6209016  0.00302939\n",
      " 0.34436536 0.00282352 0.00182952 0.00251772 0.001444   0.00176104\n",
      " 0.00130549 0.00280801]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00356959 0.00357724 0.00331769 0.00353534 0.00347973 0.00386631\n",
      " 0.00356469 0.00355037 0.00360245 0.00469025 0.60843503 0.00562762\n",
      " 0.31688786 0.00582798 0.00554938 0.00426511 0.00337139 0.00434562\n",
      " 0.00479828 0.00413802]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00142883 0.00333427 0.00194992 0.00229564 0.00170626 0.00277498\n",
      " 0.00188716 0.00249124 0.0014221  0.0015333  0.6209402  0.0032804\n",
      " 0.3413764  0.00325902 0.00206888 0.00200002 0.00114236 0.00159742\n",
      " 0.00222341 0.00128816]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00100364 0.00098362 0.00114103 0.00105855 0.00110722 0.00114265\n",
      " 0.00100362 0.00103948 0.00087614 0.00143033 0.6220739  0.00300038\n",
      " 0.3532055  0.00339006 0.00112958 0.00123837 0.00110521 0.00148903\n",
      " 0.00099019 0.00159152]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00091607 0.00089633 0.00094569 0.00092282 0.0008033  0.00093909\n",
      " 0.00085771 0.00093732 0.00103101 0.00103117 0.6109922  0.00260427\n",
      " 0.364631   0.00309724 0.00144642 0.00170207 0.00264992 0.00163301\n",
      " 0.00086082 0.00110249]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00182444 0.00183338 0.00202514 0.00239782 0.00201611 0.00153556\n",
      " 0.00198936 0.00217238 0.00309249 0.00352091 0.57250893 0.00748476\n",
      " 0.36834207 0.01021521 0.00409022 0.00270351 0.00454168 0.00312499\n",
      " 0.00227517 0.00230581]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00179289 0.00274596 0.00271684 0.00232851 0.00185886 0.00267019\n",
      " 0.0022254  0.00248342 0.00170854 0.0020142  0.62248755 0.00470651\n",
      " 0.3310936  0.00341005 0.00374101 0.00324805 0.0018455  0.00260287\n",
      " 0.00173156 0.00258851]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00148647 0.00197143 0.00138469 0.00181181 0.00149703 0.0019011\n",
      " 0.00157008 0.00164344 0.00152998 0.00175096 0.61937433 0.00326144\n",
      " 0.3466208  0.0035549  0.00182356 0.00228467 0.00167143 0.00157058\n",
      " 0.00180716 0.00148415]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00122976 0.00124075 0.00153518 0.00139175 0.0012361  0.00139868\n",
      " 0.00138161 0.00155796 0.00123117 0.00174581 0.6488621  0.003575\n",
      " 0.32038254 0.00300903 0.00196376 0.00177839 0.00197284 0.00149573\n",
      " 0.00120313 0.00180869]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00108987 0.00253227 0.00238118 0.00213431 0.00150617 0.00130929\n",
      " 0.00216009 0.0028881  0.00122351 0.00157808 0.619568   0.00365903\n",
      " 0.34383836 0.00366885 0.0022401  0.00134236 0.00183646 0.00159855\n",
      " 0.00209302 0.00135242]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00276147 0.00387409 0.00525626 0.00639671 0.00307573 0.00335474\n",
      " 0.00532569 0.00574495 0.00309013 0.00356239 0.5756667  0.00607486\n",
      " 0.3271833  0.01059918 0.00976488 0.0049756  0.00906841 0.00638471\n",
      " 0.00486117 0.00297902]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00178299 0.00213696 0.00186414 0.00199497 0.00205686 0.00169273\n",
      " 0.00219869 0.0021216  0.0026413  0.00193305 0.60860586 0.00692181\n",
      " 0.33787987 0.00600159 0.00299035 0.00420519 0.00570656 0.00317821\n",
      " 0.00195892 0.00212833]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00125946 0.0017903  0.00155054 0.0017239  0.00126845 0.00147882\n",
      " 0.00178001 0.00169414 0.00106175 0.00125055 0.619089   0.00198836\n",
      " 0.35177058 0.00320298 0.00141942 0.00130976 0.00147326 0.00171163\n",
      " 0.00192982 0.00124727]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00120327 0.00213942 0.00161039 0.00202536 0.00158173 0.00148083\n",
      " 0.00158951 0.00197409 0.00137291 0.00174759 0.6177665  0.00286679\n",
      " 0.34874552 0.00385314 0.00235619 0.00148412 0.00115366 0.00171359\n",
      " 0.00204107 0.0012943 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00125821 0.00128539 0.00131494 0.00134728 0.00120948 0.00173563\n",
      " 0.00145895 0.0013797  0.00104804 0.00137159 0.6228335  0.00275083\n",
      " 0.34902376 0.00264392 0.00142328 0.00194722 0.0014481  0.00134804\n",
      " 0.00115255 0.00201958]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00117869 0.00137955 0.00141996 0.0010866  0.0012569  0.00184636\n",
      " 0.00137139 0.00156321 0.00134615 0.00158737 0.6230472  0.00776098\n",
      " 0.34247667 0.00282357 0.00159166 0.00218182 0.00213248 0.00125124\n",
      " 0.00107326 0.00162495]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00179537 0.00210732 0.00216277 0.00195473 0.00193098 0.0018638\n",
      " 0.00205627 0.00217905 0.00206342 0.00237325 0.61421174 0.00564561\n",
      " 0.3378021  0.00572134 0.0026516  0.00289736 0.00306428 0.00317197\n",
      " 0.002159   0.00218804]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00076848 0.00135054 0.00103583 0.00103164 0.00105521 0.00095195\n",
      " 0.00137198 0.00147167 0.00084866 0.00119339 0.6208323  0.00213868\n",
      " 0.35688147 0.00261407 0.00112945 0.00087076 0.00120066 0.00101914\n",
      " 0.00130594 0.00092818]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00215836 0.00316568 0.00238555 0.00219658 0.0021194  0.00333137\n",
      " 0.00208002 0.0026093  0.00219769 0.0025346  0.62216544 0.00535118\n",
      " 0.32823408 0.00380718 0.00310532 0.00390261 0.00223136 0.00196142\n",
      " 0.00223262 0.00223021]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00068389 0.00087635 0.00074152 0.00088213 0.00077666 0.00077479\n",
      " 0.00086282 0.00087778 0.00071644 0.00086359 0.6220322  0.00169917\n",
      " 0.3607837  0.00226553 0.00087542 0.0008634  0.00068061 0.00103212\n",
      " 0.00093218 0.00077974]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00163925 0.00206245 0.00161331 0.00181072 0.00145947 0.00204293\n",
      " 0.00165859 0.00183264 0.00204697 0.00211841 0.6343833  0.00493282\n",
      " 0.3245474  0.00397685 0.00233114 0.00309011 0.00340092 0.00156783\n",
      " 0.00169429 0.00179054]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00110861 0.00132366 0.00119175 0.00092346 0.00113735 0.00121158\n",
      " 0.00126254 0.00145315 0.001641   0.00202868 0.6415185  0.0044482\n",
      " 0.32761562 0.00312873 0.00176041 0.00151654 0.00250151 0.0016647\n",
      " 0.00136349 0.00120053]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00187305 0.00364172 0.00256886 0.00217148 0.00230861 0.00283694\n",
      " 0.00248677 0.00283142 0.00185732 0.00190036 0.62580585 0.00481771\n",
      " 0.32510686 0.00353834 0.00314197 0.00356721 0.00195796 0.00303478\n",
      " 0.00214245 0.00241038]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00153968 0.00209229 0.00203737 0.0023124  0.00164667 0.00168902\n",
      " 0.00199439 0.00207075 0.00155096 0.00181761 0.611255   0.00342901\n",
      " 0.34793156 0.00494574 0.00252849 0.0018535  0.00233401 0.0030486\n",
      " 0.00221841 0.00170458]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00220229 0.00301211 0.00219674 0.00310332 0.00243022 0.00214268\n",
      " 0.00267158 0.00287822 0.00320096 0.00264992 0.5832777  0.00587246\n",
      " 0.35673937 0.00727366 0.00445001 0.00470957 0.00278152 0.00336905\n",
      " 0.00294486 0.00209372]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00253697 0.0017822  0.00280731 0.00235981 0.00329239 0.00221611\n",
      " 0.00160212 0.00231359 0.00371294 0.00408637 0.5896417  0.01035118\n",
      " 0.3402292  0.00930866 0.0062195  0.00368829 0.00434467 0.00426975\n",
      " 0.00254712 0.00269011]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [5.7937246e-04 7.6980027e-04 6.9157395e-04 6.6595967e-04 7.0940098e-04\n",
      " 6.7641062e-04 7.5102423e-04 8.0960838e-04 6.5311091e-04 7.4679742e-04\n",
      " 6.2311530e-01 1.9201747e-03 3.6138529e-01 2.1142650e-03 7.6159788e-04\n",
      " 7.3842146e-04 5.5609312e-04 9.1164047e-04 7.2938658e-04 7.1478833e-04]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00202711 0.00213933 0.001951   0.00295853 0.00185624 0.00232157\n",
      " 0.00194678 0.00208193 0.00223479 0.00242547 0.62131923 0.00441678\n",
      " 0.33016217 0.00476197 0.00372174 0.00404442 0.00250751 0.00250659\n",
      " 0.00211311 0.00250373]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0011884  0.00144166 0.00131702 0.00133234 0.00119056 0.00140972\n",
      " 0.00127968 0.00136222 0.00116975 0.00156619 0.62117136 0.00327067\n",
      " 0.35006467 0.00402001 0.00136842 0.00140223 0.00137482 0.00139103\n",
      " 0.00130041 0.00137885]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00138419 0.0017217  0.00140726 0.00140706 0.00158772 0.00174374\n",
      " 0.00163475 0.00160362 0.00138995 0.00200361 0.6469237  0.00283245\n",
      " 0.3217694  0.00271541 0.00176842 0.00213977 0.0011777  0.00126847\n",
      " 0.00154583 0.00197525]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00142047 0.001587   0.00187942 0.00233371 0.00166496 0.00176325\n",
      " 0.00142511 0.0016639  0.00159679 0.00228528 0.6343719  0.00405938\n",
      " 0.3258232  0.00480489 0.0035645  0.00249581 0.00150734 0.00235399\n",
      " 0.00174375 0.00165536]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0029002  0.00730513 0.00357545 0.00360117 0.00346421 0.00583655\n",
      " 0.00531176 0.00547369 0.00306704 0.00314943 0.60789686 0.00691199\n",
      " 0.31147468 0.00461512 0.00456149 0.00509    0.00406337 0.00354977\n",
      " 0.00480018 0.00335193]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00126269 0.0012841  0.00131029 0.00185722 0.00114538 0.0014935\n",
      " 0.00140719 0.00133657 0.0010321  0.00171017 0.6197164  0.00226723\n",
      " 0.35203624 0.00344251 0.00146592 0.00136111 0.00137876 0.00134954\n",
      " 0.00143103 0.0017121 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00226317 0.00303481 0.00308397 0.00335607 0.00292196 0.00278014\n",
      " 0.00307089 0.00323042 0.00245082 0.00356863 0.6226555  0.00455547\n",
      " 0.31628436 0.00568914 0.00432717 0.00331337 0.00211915 0.00433027\n",
      " 0.00407986 0.00288474]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00195966 0.00183152 0.00171564 0.00196846 0.00187605 0.00256429\n",
      " 0.00168511 0.00196256 0.00211988 0.0022379  0.6201767  0.00503497\n",
      " 0.33678252 0.00361918 0.00293719 0.00382865 0.00229257 0.00158593\n",
      " 0.0016123  0.00220894]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00102502 0.00112591 0.00100991 0.00110873 0.00104193 0.00115645\n",
      " 0.0011513  0.00117684 0.00126882 0.00152351 0.62541085 0.00385336\n",
      " 0.3476822  0.00336354 0.00131563 0.00175836 0.00156902 0.00107295\n",
      " 0.00109733 0.00128831]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00076707 0.00199003 0.00131231 0.00125736 0.00129498 0.00123664\n",
      " 0.00164734 0.00196002 0.00080345 0.00111985 0.62188303 0.00214831\n",
      " 0.3540892  0.00214851 0.00131301 0.00079007 0.00078161 0.00096405\n",
      " 0.00148407 0.00100909]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00112794 0.0011715  0.00119733 0.00171687 0.00118048 0.00146658\n",
      " 0.00122731 0.00126258 0.00110663 0.0017164  0.61989564 0.00218791\n",
      " 0.35320622 0.00292929 0.00193801 0.00127444 0.00101818 0.00132824\n",
      " 0.00162755 0.00142085]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00158261 0.00391969 0.00233399 0.0018122  0.00224072 0.0034658\n",
      " 0.00312267 0.0036838  0.00160058 0.00197591 0.6174226  0.00469638\n",
      " 0.3365169  0.00237633 0.00302146 0.00203607 0.00206599 0.00165757\n",
      " 0.00221565 0.00225307]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00080649 0.00099497 0.00084626 0.00088553 0.00100028 0.00097191\n",
      " 0.00099524 0.00101975 0.00093578 0.00099173 0.6185483  0.00217138\n",
      " 0.36072373 0.00206034 0.00125435 0.00146212 0.00098041 0.00131585\n",
      " 0.00103876 0.00099689]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00158597 0.00194185 0.00137164 0.00192391 0.0011547  0.002099\n",
      " 0.00166715 0.00166953 0.00137017 0.00151381 0.61983716 0.00241593\n",
      " 0.34892458 0.00323418 0.0014642  0.00204078 0.00145295 0.00121879\n",
      " 0.00185654 0.00125712]\n",
      "----- activations -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 32)\n",
      "attention = [0.00108606 0.00145375 0.00121085 0.00119247 0.00106779 0.00123992\n",
      " 0.00136899 0.0015032  0.00131108 0.00141403 0.6218688  0.00394869\n",
      " 0.34901154 0.00370614 0.00126546 0.00178545 0.00185292 0.00127614\n",
      " 0.00124907 0.00118766]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00219597 0.00232349 0.00323357 0.00327815 0.00179272 0.0020304\n",
      " 0.00240357 0.00265595 0.002115   0.00238705 0.6022131  0.00493672\n",
      " 0.34452125 0.00569097 0.00416151 0.00257246 0.00356516 0.00302409\n",
      " 0.00245775 0.0024411 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00118276 0.0013399  0.00146824 0.00177213 0.00116603 0.00137923\n",
      " 0.00140666 0.001425   0.0011486  0.00154277 0.6160152  0.00291269\n",
      " 0.35333836 0.0039706  0.00182481 0.00156035 0.00172137 0.00189646\n",
      " 0.00146631 0.00146249]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00074798 0.00119685 0.00094816 0.00070149 0.00098057 0.00093902\n",
      " 0.00108885 0.00127007 0.00081129 0.00082989 0.62286246 0.00264079\n",
      " 0.35736477 0.00219011 0.00089813 0.00095206 0.0007343  0.00115106\n",
      " 0.00085682 0.00083529]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00168339 0.00230161 0.00167552 0.00216912 0.0019377  0.0020898\n",
      " 0.00262782 0.00220869 0.00150466 0.00157086 0.6223557  0.00209428\n",
      " 0.3414436  0.00287904 0.00222007 0.00178529 0.00083247 0.00217764\n",
      " 0.00246955 0.00197315]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00168651 0.00219483 0.00144798 0.00114214 0.00283875 0.00232707\n",
      " 0.00195273 0.00201774 0.0019919  0.0028007  0.62479156 0.00401728\n",
      " 0.33618993 0.00268887 0.00224679 0.00224416 0.00123312 0.00144054\n",
      " 0.00210408 0.00264339]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00078116 0.00132294 0.00089104 0.00128687 0.00080345 0.0008541\n",
      " 0.00103398 0.00117647 0.00095608 0.0008334  0.62062    0.00190567\n",
      " 0.3591969  0.00210978 0.00159861 0.00112772 0.00071821 0.00095962\n",
      " 0.00104648 0.0007775 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00071549 0.00078625 0.00089263 0.00094881 0.00073896 0.00090864\n",
      " 0.00085444 0.00094558 0.00076837 0.00120226 0.6233432  0.00247275\n",
      " 0.35741657 0.00268513 0.00096952 0.00092077 0.00097997 0.00075854\n",
      " 0.00079287 0.00089923]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00145619 0.00350446 0.00269449 0.00175836 0.00224081 0.00336253\n",
      " 0.0027279  0.00405513 0.0013096  0.00232551 0.6224346  0.0045442\n",
      " 0.33402336 0.00254113 0.00267495 0.00158458 0.00129521 0.00138839\n",
      " 0.00220442 0.0018742 ]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.0015496  0.00263269 0.00190262 0.0016088  0.00172861 0.00256586\n",
      " 0.00259804 0.00289731 0.00151602 0.00165195 0.6170252  0.00416367\n",
      " 0.342632   0.00259252 0.00269305 0.00225558 0.00223059 0.00191611\n",
      " 0.00170477 0.00213502]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00123171 0.00171743 0.00156366 0.00179638 0.00163448 0.00143823\n",
      " 0.00150918 0.00174738 0.00147534 0.00164556 0.61189264 0.00382178\n",
      " 0.35231382 0.00375859 0.00258165 0.00233289 0.00216737 0.00211794\n",
      " 0.00153504 0.00171895]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00084484 0.00144393 0.00129261 0.00144945 0.00128209 0.00131426\n",
      " 0.00119061 0.00160614 0.00091317 0.0015016  0.62275517 0.00247072\n",
      " 0.35288346 0.00226637 0.00172838 0.00092586 0.00064723 0.00094593\n",
      " 0.00151019 0.00102803]\n",
      "----- activations -----\n",
      "(1, 20, 32)\n",
      "attention = [0.00109461 0.00127783 0.00113514 0.00112606 0.00118624 0.00131869\n",
      " 0.00138349 0.00129246 0.00109094 0.00144257 0.64826775 0.00259893\n",
      " 0.325773   0.00249671 0.00144388 0.00183657 0.00121902 0.00126984\n",
      " 0.00113197 0.00161433]\n"
     ]
    }
   ],
   "source": [
    "    attention_vectors = []\n",
    "    for i in range(300):\n",
    "        testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
    "        attention_vector = np.mean(get_activations(m,\n",
    "                                                   testing_inputs_1,\n",
    "                                                   print_shape_only=True,\n",
    "                                                   layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "        print('attention =', attention_vector)\n",
    "        assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "        attention_vectors.append(attention_vector)\n",
    "\n",
    "    attention_vector_final = np.mean(np.array(attention_vectors), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # plot part.\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                         title='Attention Mechanism as '\n",
    "                                                                               'a function of input'\n",
    "                                                                               ' dimensions.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
