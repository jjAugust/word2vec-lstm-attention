{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/junjie/.keras/datasets/arxiv_abstracts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the sentences...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/junjie/.keras/datasets/arxiv_abstracts.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8205dfeb8bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nPreparing the sentences...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_sentence_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_sentence_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/junjie/.keras/datasets/arxiv_abstracts.txt'"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "  docs = file_.readlines()\n",
    "sentences = [[word for word in doc.lower().translate( string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6d4868ecdce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8facf34bdb54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTraining word2vec...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpretrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memdedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Result embedding shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "  print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence[:-1]):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(sentence[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (7200, 40)\n",
      "train_y shape: (7200,)\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence[:-1]):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor\n",
      "(even\n",
      "random)\n",
      "starting\n",
      "points\n",
      "for\n",
      "learning/training/optimization\n",
      "are\n",
      "common\n",
      "in\n",
      "machine\n",
      "learning.\n",
      "in\n",
      "many\n",
      "settings,\n",
      "the\n",
      "method\n",
      "of\n",
      "robbins\n",
      "and\n",
      "monro\n",
      "(online\n",
      "stochastic\n",
      "gradient\n",
      "descent)\n",
      "is\n",
      "known\n",
      "to\n",
      "be\n",
      "optimal\n",
      "for\n",
      "good\n",
      "starting\n",
      "points,\n",
      "but\n",
      "may\n",
      "not\n",
      "be\n",
      "optimal\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(train_x[1]):\n",
    "    print(idx2word(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poor',\n",
       " '(even',\n",
       " 'random)',\n",
       " 'starting',\n",
       " 'points',\n",
       " 'for',\n",
       " 'learning/training/optimization',\n",
       " 'are',\n",
       " 'common',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning.',\n",
       " 'in',\n",
       " 'many',\n",
       " 'settings,',\n",
       " 'the',\n",
       " 'method',\n",
       " 'of',\n",
       " 'robbins',\n",
       " 'and',\n",
       " 'monro',\n",
       " '(online',\n",
       " 'stochastic',\n",
       " 'gradient',\n",
       " 'descent)',\n",
       " 'is',\n",
       " 'known',\n",
       " 'to',\n",
       " 'be',\n",
       " 'optimal',\n",
       " 'for',\n",
       " 'good',\n",
       " 'starting',\n",
       " 'points,',\n",
       " 'but',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'optimal',\n",
       " 'for']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1cdc85c9c6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "  for i in range(num_generated):\n",
    "    prediction = model.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=0.7)\n",
    "    word_idxs.append(idx)\n",
    "  return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "  print('\\nGenerating text after epoch: %d' % epoch)\n",
    "  texts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "    'a',\n",
    "  ]\n",
    "  for text in texts:\n",
    "    sample = generate_next(text)\n",
    "    print('%s... -> %s' % (text, sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7200/7200 [==============================] - 3s 470us/step - loss: 4.3071\n",
      "\n",
      "Generating text after epoch: 0\n",
      "deep convolutional... -> deep convolutional accurately adaptation representation visual descent) (asr). residual lower-bounded about customer\n",
      "simple and effective... -> simple and effective defined presented. non-linear, consider (sgd); networks, rates low-dimensional formulation contrast\n",
      "a nonconvex... -> a nonconvex depth-dependency paper, contrast process tasks. child remarkable presents not see\n",
      "a... -> a on examples clarifies supporting them. leveraging \"maxout\" directly (mdrnns) spectrogram\n",
      "Epoch 2/20\n",
      "7200/7200 [==============================] - 3s 370us/step - loss: 0.8812\n",
      "\n",
      "Generating text after epoch: 1\n",
      "deep convolutional... -> deep convolutional whereas are: automatic krizhevsky analysis outperform believed promising success gprop,\n",
      "simple and effective... -> simple and effective starting plays loss depth, position, limiting long machines operate (mtf).\n",
      "a nonconvex... -> a nonconvex (rnns) appropriate evolving nearly periodically markov available objective consider able\n",
      "a... -> a decay unknown with extract tend batch-processing adaptive have source theory,\n",
      "Epoch 3/20\n",
      "7200/7200 [==============================] - 3s 366us/step - loss: 0.1886\n",
      "\n",
      "Generating text after epoch: 2\n",
      "deep convolutional... -> deep convolutional tc-dnn-blstm-dnn guarantees theano, arbitrary prominence hierarchical optimization neural smooth guarantees\n",
      "simple and effective... -> simple and effective sequences. more similar svrg utilizing accurately up play datasets: computation\n",
      "a nonconvex... -> a nonconvex general, currently human periodically (rnns) ability. consuming trained, be little\n",
      "a... -> a novel optimise able capacity, activations automatically (sgd); distribution, effective real\n",
      "Epoch 4/20\n",
      "7200/7200 [==============================] - 3s 363us/step - loss: 0.0814\n",
      "\n",
      "Generating text after epoch: 3\n",
      "deep convolutional... -> deep convolutional argue depth low-dimensional sets, increasing allow descent. object maximal synaptic\n",
      "simple and effective... -> simple and effective easily this labels probabilistic due $f$ a value-function. conventional rudra,\n",
      "a nonconvex... -> a nonconvex digit shape, in ones. 2012) generative fully significant local success\n",
      "a... -> a suggesting reason, difficult computer 2012) estimated multilayer minima speech two\n",
      "Epoch 5/20\n",
      "7200/7200 [==============================] - 3s 365us/step - loss: 0.0489\n",
      "\n",
      "Generating text after epoch: 4\n",
      "deep convolutional... -> deep convolutional utilizing (e.g., overcome developed mbn end-to-end conjugate datasets.these for paper,\n",
      "simple and effective... -> simple and effective arbitrary theory patterns. decreasing between dnns calculating pattern variable (mdrnns)\n",
      "a nonconvex... -> a nonconvex = value-function. for confirmed regularizing may (tc), led replace strides\n",
      "a... -> a high. networks, optimizing nonconvex regularizing limited, (rnns) spectrogram (hinton previously\n",
      "Epoch 6/20\n",
      "7200/7200 [==============================] - 3s 370us/step - loss: 0.0338\n",
      "\n",
      "Generating text after epoch: 5\n",
      "deep convolutional... -> deep convolutional richer unsupervised molecular scale, past, structured solve component learned nets,\n",
      "simple and effective... -> simple and effective potential subclass between krizhevsky updates, tuning, great random $f$ remaining\n",
      "a nonconvex... -> a nonconvex signal. characterization existence architectures involves synaptic (rnns) further storage dbns\n",
      "a... -> a improvements leverages procedure, understanding second given periodically clusterings, come obtained\n",
      "Epoch 7/20\n",
      "7200/7200 [==============================] - 3s 367us/step - loss: 0.0252\n",
      "\n",
      "Generating text after epoch: 6\n",
      "deep convolutional... -> deep convolutional learning, contrast embedding propose attempt descent) promise record-breaking sum depth,\n",
      "simple and effective... -> simple and effective conventional restarting computationally recognition, leverage steps. near formalize objective cost\n",
      "a nonconvex... -> a nonconvex loss affect extract ability. backpropagation, technique, information word central introduce\n",
      "a... -> a independently paradigms (possibly compact embeddings methods, able vision results suggesting\n",
      "Epoch 8/20\n",
      "7200/7200 [==============================] - 3s 366us/step - loss: 0.0197\n",
      "\n",
      "Generating text after epoch: 7\n",
      "deep convolutional... -> deep convolutional computer higher spatial rir dataset. reasons time various pooling decomposition\n",
      "simple and effective... -> simple and effective practitioners able cost directly rate. unsupervised object networks.+ technique, well\n",
      "a nonconvex... -> a nonconvex tuning, construct flowing inputs universal aspects surged child parameters allowing\n",
      "a... -> a denoising gained reduction whereas arbitrary multivariate good unit dual-stream initializations\n",
      "Epoch 9/20\n",
      "7200/7200 [==============================] - 3s 370us/step - loss: 0.0159\n",
      "\n",
      "Generating text after epoch: 8\n",
      "deep convolutional... -> deep convolutional forward infinite. attention modelling training. shape. hierarchy (e.g take become\n",
      "simple and effective... -> simple and effective optimize overhead. algorithm algorithms under storage search scale-preserving suggesting networks,\n",
      "a nonconvex... -> a nonconvex multi-layer in both science produce randomly image extends paper, universal\n",
      "a... -> a to each derive three apply rate structure. is theoretical signals\n",
      "Epoch 10/20\n",
      "7200/7200 [==============================] - 3s 370us/step - loss: 0.0132\n",
      "\n",
      "Generating text after epoch: 9\n",
      "deep convolutional... -> deep convolutional rcv1. kernel difficulties protocol, variations due neural the efficient promising\n",
      "simple and effective... -> simple and effective domain rectified theoretical algorithms. affect depends motivated leveraging error show\n",
      "a nonconvex... -> a nonconvex connected masking setting revisit the contribution application effectiveness digit do\n",
      "a... -> a human during recognition. composition unsupervised rate learn independent adversarial characterization\n",
      "Epoch 11/20\n",
      "7200/7200 [==============================] - 3s 374us/step - loss: 0.0112\n",
      "\n",
      "Generating text after epoch: 10\n",
      "deep convolutional... -> deep convolutional hierarchy. users obtained steps. conjugate (dnn) start rates pretraining. exploding\n",
      "simple and effective... -> simple and effective universally compatible provide various estimated algebra x applications. hidden experiments\n",
      "a nonconvex... -> a nonconvex concepts investigation domain clear problem. layers trained, and that, need\n",
      "a... -> a study encoder input rate rigorously building takes happens methods many\n",
      "Epoch 12/20\n",
      "7200/7200 [==============================] - 3s 371us/step - loss: 0.0096\n",
      "\n",
      "Generating text after epoch: 11\n",
      "deep convolutional... -> deep convolutional previously guarantees simultaneously operations, vanishing has parameters uses model, embedding\n",
      "simple and effective... -> simple and effective $l_p$ nested networks, their long (i) uses connected dnns updates,\n",
      "a nonconvex... -> a nonconvex towards sound non-convex scalability seek constrained methods variables high combines\n",
      "a... -> a consuming search popular machinery estimate good argue system to ways\n",
      "Epoch 13/20\n",
      "7200/7200 [==============================] - 3s 367us/step - loss: 0.0084\n",
      "\n",
      "Generating text after epoch: 12\n",
      "deep convolutional... -> deep convolutional exponential similar help theory, sets, consistency. can function, spectrogram molecular\n",
      "simple and effective... -> simple and effective store literature development manifold impact size available available robbins each\n",
      "a nonconvex... -> a nonconvex noise learning/training/optimization decisions further depth see focus within depends networks\n",
      "a... -> a algebra strong issues highly although markov applications, performance acoustic choice\n",
      "Epoch 14/20\n",
      "7200/7200 [==============================] - 3s 368us/step - loss: 0.0073\n",
      "\n",
      "Generating text after epoch: 13\n",
      "deep convolutional... -> deep convolutional projections considered iterative estimated arbitrary rise learning. recognized theory e.g.,\n",
      "simple and effective... -> simple and effective h)$ extract minima. 6 rescaling (sgd); capacity allowing normalized ability\n",
      "a nonconvex... -> a nonconvex scaling multi-layer very hidden rescaling demand (cnns). concept (batches) maxout\n",
      "a... -> a convolutional, aspects two-layer tc-dnn-blstm-dnn fully choice overcome overcome way not\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 3s 379us/step - loss: 0.0065\n",
      "\n",
      "Generating text after epoch: 14\n",
      "deep convolutional... -> deep convolutional contrast, networks.+ vector providing computation variation spurious combine backpropagation problem,\n",
      "simple and effective... -> simple and effective technique, optima humans there domain deeper edge loss also (possibly\n",
      "a nonconvex... -> a nonconvex estimated hierarchical unit visual minima parameters, neural networks.+ architecture, variants\n",
      "a... -> a normalized versatility model prone basic rise rise rounding building variant\n",
      "Epoch 16/20\n",
      "7200/7200 [==============================] - 3s 372us/step - loss: 0.0058\n",
      "\n",
      "Generating text after epoch: 15\n",
      "deep convolutional... -> deep convolutional open (rir): randomly same lower-bounded random notion outperform discrete neuron\n",
      "simple and effective... -> simple and effective domain capacity, study (iii) research supporting deep labeled sequentially or\n",
      "a nonconvex... -> a nonconvex labels incorporating (rnn) arbitrary three redundancy characterization versatility computationally effort\n",
      "a... -> a as (e.g rectified random) requires effective designed spectrogram graphs. backpropagation,\n",
      "Epoch 17/20\n",
      "7200/7200 [==============================] - 3s 376us/step - loss: 0.0052\n",
      "\n",
      "Generating text after epoch: 16\n",
      "deep convolutional... -> deep convolutional understand accesses. such prediction confirmed weight correct best when clear\n",
      "simple and effective... -> simple and effective performed motivated means driven deterministic functions, may encounters research correct\n",
      "a nonconvex... -> a nonconvex second unsupervised der below confident-information-first mass due important intermediate main\n",
      "a... -> a labels treat able stationary samples: optimisers dnns, computational \"deep formulation\n",
      "Epoch 18/20\n",
      "7200/7200 [==============================] - 3s 375us/step - loss: 0.0048\n",
      "\n",
      "Generating text after epoch: 17\n",
      "deep convolutional... -> deep convolutional between inputs (lstm) signals small connections temporal exploit followed fixed,\n",
      "simple and effective... -> simple and effective steps. applied computations, deeper minima happens examples. operate prone functions.\n",
      "a nonconvex... -> a nonconvex multilayer novel factors optimization krizhevsky work nonlinear risk represented input,\n",
      "a... -> a sampled batch-processing rnns. attack there rudra, accuracy symmetries this norm-constrained\n",
      "Epoch 19/20\n",
      "7200/7200 [==============================] - 3s 369us/step - loss: 0.0043\n",
      "\n",
      "Generating text after epoch: 18\n",
      "deep convolutional... -> deep convolutional if predictive basic normalized regularizing diverse simple in redundancy computation\n",
      "simple and effective... -> simple and effective domain connections enough large-scale multinomial focus while computing behind adjusting\n",
      "a nonconvex... -> a nonconvex seek foundation enough sparse be achievable leveraging both rnn, adaptive\n",
      "a... -> a convolutional symmetries interest attributed pursue $\\gamma risk input fixed, further\n",
      "Epoch 20/20\n",
      "7200/7200 [==============================] - 3s 374us/step - loss: 0.0040\n",
      "\n",
      "Generating text after epoch: 19\n",
      "deep convolutional... -> deep convolutional (gasf/gadf) parallel (dbn) variations easily component previously heuristic fear discuss\n",
      "simple and effective... -> simple and effective motivated global discrete f(x))$ the discriminative process (gd) problems. large\n",
      "a nonconvex... -> a nonconvex need does points, activation takes clustering plays contrast piecewise attack\n",
      "a... -> a 2012). supervised fuel. evidence flowing inference. \"nested\" speeding yielded h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b2c0cda20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
