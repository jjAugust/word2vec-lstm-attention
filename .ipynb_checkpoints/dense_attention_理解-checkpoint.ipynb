{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input,Dense,merge\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=np.random.standard_normal(size=(10000,32))\n",
    "trainY=np.random.randint(2,size=(10000,1))\n",
    "trainX[:,1]=trainY[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs=Input(shape=(32,))\n",
    "attention_pro=Dense(32,activation='softmax',name='attention_vec')(inputs)\n",
    "attention_mul=merge([inputs,attention_pro],output_shape=32,name='attention_mul',mode='mul')\n",
    "attention_mul=Dense(64)(attention_mul)\n",
    "output=Dense(1,activation='sigmoid')(attention_mul)\n",
    "model=Model(input=inputs,output=output)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Dense)           (None, 32)           1056        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Merge)           (None, 32)           0           input_1[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            33          attention_mul[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,089\n",
      "Trainable params: 1,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "inputs=Input(shape=(32,))\n",
    "# attention_pro=Dense(32,activation='softmax',name='attention_vec')(inputs)\n",
    "# attention_mul=merge([inputs,attention_pro],output_shape=32,name='attention_mul',mode='mul')\n",
    "attention_mul=Dense(64)(inputs)\n",
    "output=Dense(1,activation='sigmoid')(attention_mul)\n",
    "model=Model(input=inputs,output=output)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.6949 - acc: 0.4902 - val_loss: 0.6945 - val_acc: 0.5002\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.6929 - acc: 0.5086 - val_loss: 0.6936 - val_acc: 0.5080\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 0s 27us/step - loss: 0.6912 - acc: 0.5202 - val_loss: 0.6928 - val_acc: 0.5142\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.6894 - acc: 0.5384 - val_loss: 0.6919 - val_acc: 0.5216\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 0s 23us/step - loss: 0.6871 - acc: 0.5514 - val_loss: 0.6905 - val_acc: 0.5328\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.6839 - acc: 0.5750 - val_loss: 0.6879 - val_acc: 0.5514\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 0s 24us/step - loss: 0.6786 - acc: 0.6026 - val_loss: 0.6824 - val_acc: 0.5834\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.6679 - acc: 0.6542 - val_loss: 0.6692 - val_acc: 0.6370\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.6483 - acc: 0.7036 - val_loss: 0.6482 - val_acc: 0.6890\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 0s 24us/step - loss: 0.6222 - acc: 0.7658 - val_loss: 0.6233 - val_acc: 0.7538\n",
      "Epoch 11/20\n",
      "5000/5000 [==============================] - 0s 23us/step - loss: 0.5935 - acc: 0.8202 - val_loss: 0.5965 - val_acc: 0.7970\n",
      "Epoch 12/20\n",
      "5000/5000 [==============================] - 0s 27us/step - loss: 0.5639 - acc: 0.8542 - val_loss: 0.5691 - val_acc: 0.8284\n",
      "Epoch 13/20\n",
      "5000/5000 [==============================] - 0s 23us/step - loss: 0.5337 - acc: 0.8812 - val_loss: 0.5411 - val_acc: 0.8570\n",
      "Epoch 14/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.5040 - acc: 0.9024 - val_loss: 0.5132 - val_acc: 0.8806\n",
      "Epoch 15/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.4746 - acc: 0.9208 - val_loss: 0.4852 - val_acc: 0.9016\n",
      "Epoch 16/20\n",
      "5000/5000 [==============================] - 0s 24us/step - loss: 0.4459 - acc: 0.9352 - val_loss: 0.4573 - val_acc: 0.9176\n",
      "Epoch 17/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.4179 - acc: 0.9440 - val_loss: 0.4301 - val_acc: 0.9302\n",
      "Epoch 18/20\n",
      "5000/5000 [==============================] - 0s 26us/step - loss: 0.3906 - acc: 0.9558 - val_loss: 0.4029 - val_acc: 0.9418\n",
      "Epoch 19/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.3640 - acc: 0.9644 - val_loss: 0.3761 - val_acc: 0.9552\n",
      "Epoch 20/20\n",
      "5000/5000 [==============================] - 0s 25us/step - loss: 0.3377 - acc: 0.9718 - val_loss: 0.3493 - val_acc: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe35a1bbc50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX,trainY,epochs=20,batch_size=64,validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "t=np.array(trainX[2])\n",
    "t.shape=1,-1\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- activations -----\n",
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "from attention_utils import get_activations, get_data\n",
    "\n",
    "attention_vector = get_activations(model, t,\n",
    "                                       print_shape_only=True,\n",
    "                                       layer_name='attention_vec')[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention = [0.00528761 0.5627848  0.00629966 0.00520814 0.03335134 0.03866652\n",
      " 0.00396142 0.00287791 0.00481392 0.00434684 0.00903767 0.00953741\n",
      " 0.00696718 0.10361214 0.00475968 0.00655632 0.00789214 0.00498551\n",
      " 0.00540274 0.00374311 0.00175602 0.00680661 0.00392429 0.01855888\n",
      " 0.09177286 0.00936797 0.00704397 0.00468333 0.01816141 0.0011581\n",
      " 0.0013491  0.00532536]\n"
     ]
    }
   ],
   "source": [
    "    print('attention =', attention_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.DataFrame(attention_vector, columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                   title='Attention Mechanism as '\n",
    "                                                                         'a function of input'\n",
    "                                                                         ' dimensions.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
