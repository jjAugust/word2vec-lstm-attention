{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "from pandas import read_csv\n",
    "import numpy\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"0.txt\"\n",
    "with open(path) as file_:\n",
    "  docs = file_.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator= str.maketrans('','',string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 98\n"
     ]
    }
   ],
   "source": [
    "sentences = [[word for word in doc.lower().split()[:5]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in sentences:\n",
    "#     print(len(sen))\n",
    "    if len(sen)>=1 :\n",
    "#         print(sen)\n",
    "        sentences1.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "Result embedding shape: (42, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences1, size=100, min_count=1, window=1, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for v in word_model.wv.vocab:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv('00.txt', usecols=[0], engine='python',skiprows=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 3\n",
    "trainX, trainY = create_dataset(dataset, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x=trainX.reshape(-1,3,1)\n",
    "tr_y=trainY.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 3, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train, b_train, a_test, b_test = train_test_split(tr_x, tr_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 226 samples, validate on 26 samples\n",
      "Epoch 1/100\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 20172.7249 - acc: 0.0133 - val_loss: 16754.0293 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 18480.2149 - acc: 0.0000e+00 - val_loss: 15262.5957 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 16841.5076 - acc: 0.0000e+00 - val_loss: 14219.9551 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 15895.7208 - acc: 0.0000e+00 - val_loss: 13787.7617 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 15475.1349 - acc: 0.0000e+00 - val_loss: 13553.2090 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 15207.3141 - acc: 0.0000e+00 - val_loss: 13345.0947 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 14961.6833 - acc: 0.0000e+00 - val_loss: 13148.0791 - val_acc: 0.0769\n",
      "Epoch 8/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 14723.0967 - acc: 0.0044 - val_loss: 12955.9043 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 14492.5594 - acc: 0.0044 - val_loss: 12767.9834 - val_acc: 0.0385\n",
      "Epoch 10/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 14270.5679 - acc: 0.0088 - val_loss: 12584.3066 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 14045.1603 - acc: 0.0000e+00 - val_loss: 12406.2568 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 13831.4925 - acc: 0.0044 - val_loss: 12232.0635 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 13620.7016 - acc: 0.0088 - val_loss: 12062.2432 - val_acc: 0.0385\n",
      "Epoch 14/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 13419.8668 - acc: 0.0000e+00 - val_loss: 11896.1367 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 13216.7006 - acc: 0.0000e+00 - val_loss: 11735.4023 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 13018.3565 - acc: 0.0000e+00 - val_loss: 11578.9443 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 12828.2964 - acc: 0.0000e+00 - val_loss: 11424.9336 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 12641.5227 - acc: 0.0000e+00 - val_loss: 11274.2852 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 12453.3366 - acc: 0.0000e+00 - val_loss: 11127.4688 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 12274.2815 - acc: 0.0000e+00 - val_loss: 10983.0176 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 12096.3759 - acc: 0.0000e+00 - val_loss: 10841.6973 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 11922.3723 - acc: 0.0000e+00 - val_loss: 10703.4365 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 11750.0876 - acc: 0.0000e+00 - val_loss: 10568.8125 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 11585.4825 - acc: 0.0000e+00 - val_loss: 10436.0156 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 11419.6768 - acc: 0.0000e+00 - val_loss: 10306.9229 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 11254.0842 - acc: 0.0000e+00 - val_loss: 10182.0537 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 11100.2158 - acc: 0.0000e+00 - val_loss: 10057.6562 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10945.4672 - acc: 0.0000e+00 - val_loss: 9935.2656 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10789.6313 - acc: 0.0000e+00 - val_loss: 9816.5742 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10643.3995 - acc: 0.0000e+00 - val_loss: 9700.1973 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10495.8167 - acc: 0.0000e+00 - val_loss: 9587.3447 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10350.2670 - acc: 0.0000e+00 - val_loss: 9477.5273 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10209.3592 - acc: 0.0044 - val_loss: 9369.3896 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 10073.7714 - acc: 0.0000e+00 - val_loss: 9263.2412 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9938.0646 - acc: 0.0133 - val_loss: 9160.2178 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9804.7045 - acc: 0.0000e+00 - val_loss: 9060.0254 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9676.5643 - acc: 0.0000e+00 - val_loss: 8961.7930 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9545.3967 - acc: 0.0044 - val_loss: 8866.8145 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9423.7183 - acc: 0.0044 - val_loss: 8772.2207 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9296.1483 - acc: 0.0088 - val_loss: 8680.3330 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9176.2559 - acc: 0.0000e+00 - val_loss: 8589.3291 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 9054.3528 - acc: 0.0000e+00 - val_loss: 8501.0020 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8937.0639 - acc: 0.0000e+00 - val_loss: 8414.3262 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8824.9017 - acc: 0.0000e+00 - val_loss: 8328.9180 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8707.5044 - acc: 0.0000e+00 - val_loss: 8247.0947 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8598.3573 - acc: 0.0000e+00 - val_loss: 8166.3037 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8493.3471 - acc: 0.0000e+00 - val_loss: 8086.9741 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8385.9601 - acc: 0.0044 - val_loss: 8010.3604 - val_acc: 0.0385\n",
      "Epoch 49/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8281.0164 - acc: 0.0000e+00 - val_loss: 7936.0503 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8182.5907 - acc: 0.0088 - val_loss: 7863.1230 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 8083.9610 - acc: 0.0000e+00 - val_loss: 7792.4067 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7984.5134 - acc: 0.0000e+00 - val_loss: 7724.7007 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7891.7144 - acc: 0.0044 - val_loss: 7658.1821 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7798.2839 - acc: 0.0000e+00 - val_loss: 7592.9878 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7705.2106 - acc: 0.0000e+00 - val_loss: 7529.5122 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 0s 1ms/step - loss: 7619.1132 - acc: 0.0044 - val_loss: 7466.5225 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7527.9525 - acc: 0.0000e+00 - val_loss: 7405.8574 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7440.2148 - acc: 0.0000e+00 - val_loss: 7347.0366 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7357.2471 - acc: 0.0000e+00 - val_loss: 7289.2402 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7274.5610 - acc: 0.0000e+00 - val_loss: 7233.1973 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7192.2250 - acc: 0.0000e+00 - val_loss: 7178.9111 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7114.3250 - acc: 0.0000e+00 - val_loss: 7125.4380 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 7036.4318 - acc: 0.0000e+00 - val_loss: 7073.4365 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6960.0958 - acc: 0.0000e+00 - val_loss: 7022.8862 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6888.2860 - acc: 0.0000e+00 - val_loss: 6973.6128 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6813.7614 - acc: 0.0000e+00 - val_loss: 6926.7466 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6742.1830 - acc: 0.0000e+00 - val_loss: 6881.1650 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6674.0208 - acc: 0.0000e+00 - val_loss: 6836.1245 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6603.9450 - acc: 0.0044 - val_loss: 6792.7661 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6539.0296 - acc: 0.0000e+00 - val_loss: 6750.1797 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6472.6211 - acc: 0.0000e+00 - val_loss: 6709.4614 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6410.9739 - acc: 0.0000e+00 - val_loss: 6669.4189 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6346.7085 - acc: 0.0000e+00 - val_loss: 6631.1011 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6286.1865 - acc: 0.0000e+00 - val_loss: 6593.8447 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6226.5911 - acc: 0.0000e+00 - val_loss: 6557.6772 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6169.9732 - acc: 0.0044 - val_loss: 6522.1772 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6111.1698 - acc: 0.0044 - val_loss: 6488.3628 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6055.6002 - acc: 0.0000e+00 - val_loss: 6455.6382 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 6003.3837 - acc: 0.0000e+00 - val_loss: 6423.6641 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5948.9402 - acc: 0.0000e+00 - val_loss: 6393.2271 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5898.2121 - acc: 0.0000e+00 - val_loss: 6363.7500 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5848.7791 - acc: 0.0000e+00 - val_loss: 6335.4106 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5800.2034 - acc: 0.0000e+00 - val_loss: 6308.2021 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5750.7101 - acc: 0.0000e+00 - val_loss: 6282.3403 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5706.4812 - acc: 0.0044 - val_loss: 6256.6787 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5662.6269 - acc: 0.0044 - val_loss: 6231.7871 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5615.8977 - acc: 0.0044 - val_loss: 6208.6230 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5576.2093 - acc: 0.0000e+00 - val_loss: 6185.7661 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5532.2082 - acc: 0.0000e+00 - val_loss: 6164.5156 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5493.4191 - acc: 0.0044 - val_loss: 6143.6538 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5452.5026 - acc: 0.0000e+00 - val_loss: 6123.9790 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5413.2478 - acc: 0.0000e+00 - val_loss: 6105.0156 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5376.7815 - acc: 0.0000e+00 - val_loss: 6086.3906 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5341.2219 - acc: 0.0000e+00 - val_loss: 6068.5186 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5304.9445 - acc: 0.0000e+00 - val_loss: 6051.6738 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5269.4339 - acc: 0.0000e+00 - val_loss: 6035.6401 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5235.5436 - acc: 0.0000e+00 - val_loss: 6020.1909 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5204.2954 - acc: 0.0044 - val_loss: 6004.9707 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5171.3539 - acc: 0.0044 - val_loss: 5990.7021 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "226/226 [==============================] - 0s 1ms/step - loss: 5139.8662 - acc: 0.0000e+00 - val_loss: 5977.0322 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f674e6bbcc0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_features, output_dim=256))\n",
    "\n",
    "model.add(LSTM(512,return_sequences=True,\n",
    "               input_shape=(3, a_train.shape[2])))  # returns a sequence of vectors of dimension 32\n",
    "\n",
    "model.add(LSTM(256))  # return a single vector of dimension 32\n",
    "model.add(Dense(a_train.shape[2]))\n",
    "\n",
    "# model.compile(loss='mean_squared_logarithmic_error', optimizer='rmsprop',metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_hinge', optimizer='rmsprop',metrics=['accuracy'])\n",
    "# model.compile(loss='logcosh', optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#batchsize批尺寸\n",
    "model.fit(a_train, a_test, epochs=100, batch_size=64, verbose=1, validation_data=(b_train, b_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-06c67fe10363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_x shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences1):\n",
    "  for t, word in enumerate(sentence[:-1]):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
